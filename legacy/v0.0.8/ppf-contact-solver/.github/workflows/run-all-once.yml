# File: run-all-once.yml
# Code: Claude Code and Codex
# Review: Ryoichi Ando (ryoichi.ando@zozo.com)
# License: Apache v2.0
# Generated by: run-all-once-gen.py

name: All Examples

on:
  workflow_dispatch:
    inputs:
      instance_type:
        description: 'EC2 instance type'
        required: true
        default: 'g6e.2xlarge'
        type: choice
        options:
          - g6.2xlarge
          - g6e.2xlarge
      region:
        description: 'AWS Region'
        required: true
        default: 'us-east-1'
        type: choice
        options:
          - us-east-1
          - us-east-2

jobs:
  run-batch-1:
    name: Run Batch 1
    runs-on: ubuntu-latest
    permissions:
      id-token: write
      contents: read

    env:
      AWS_REGION: ${{ github.event.inputs.region }}
      INSTANCE_TYPE: ${{ github.event.inputs.instance_type }}
      BRANCH: ${{ github.ref_name }}
      EXAMPLES: "belt cards codim curtain domino"
      WORKDIR: /home/ubuntu
      USER: ubuntu

    steps:
      - name: Show input parameters
        run: |
          echo "## Input Parameters - Batch 1"
          echo "Branch: ${{ github.ref_name }}"
          echo "Instance Type: ${{ github.event.inputs.instance_type }}"
          echo "Region: ${{ github.event.inputs.region }}"
          echo "Examples: belt cards codim curtain domino"

      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Verify AWS authentication
        run: |
          echo "Testing AWS authentication..."
          aws sts get-caller-identity
          echo "AWS Region: $AWS_REGION"
          echo "Instance Type: $INSTANCE_TYPE"
          echo "Branch: $BRANCH"
          echo "Examples: $EXAMPLES"

      - name: Get GitHub Actions runner public IP
        id: runner-ip
        run: |
          echo "Fetching GitHub Actions runner public IP..."
          RUNNER_IP=$(curl -s --max-time 10 https://checkip.amazonaws.com | tr -d '\n')
          if [ -z "$RUNNER_IP" ]; then
            echo "ERROR: Failed to get IP from checkip.amazonaws.com"
            exit 1
          fi
          echo "::add-mask::$RUNNER_IP"
          echo "RUNNER_IP=$RUNNER_IP" >> $GITHUB_OUTPUT
          echo "GitHub Actions Runner IP: $RUNNER_IP"

      - name: Find Deep Learning AMI
        id: ami
        run: |
          echo "Finding latest Deep Learning AMI with GPU support..."
          AMI_ID=$(aws ec2 describe-images \
            --owners amazon \
            --filters \
              "Name=name,Values=Deep Learning Base OSS Nvidia Driver GPU AMI (Ubuntu 24.04)*" \
              "Name=state,Values=available" \
              "Name=architecture,Values=x86_64" \
            --query 'sort_by(Images, &CreationDate)[-1].ImageId' \
            --region "$AWS_REGION" \
            --output text)

          if [ "$AMI_ID" = "None" ] || [ -z "$AMI_ID" ]; then
            echo "ERROR: Deep Learning AMI not found in region $AWS_REGION"
            echo "This workflow requires the Deep Learning AMI with pre-installed NVIDIA drivers"
            echo "Please check if the AMI is available in your selected region"
            exit 1
          fi

          echo "AMI_ID=$AMI_ID" >> $GITHUB_OUTPUT
          echo "Found AMI: $AMI_ID"

      - name: Get default VPC ID
        id: vpc
        run: |
          echo "Getting default VPC ID..."
          VPC_ID=$(aws ec2 describe-vpcs \
            --filters "Name=isDefault,Values=true" \
            --query 'Vpcs[0].VpcId' \
            --region "$AWS_REGION" \
            --output text)

          if [ "$VPC_ID" = "None" ] || [ -z "$VPC_ID" ]; then
            echo "ERROR: Default VPC not found in region $AWS_REGION"
            exit 1
          fi

          echo "VPC_ID=$VPC_ID" >> $GITHUB_OUTPUT
          echo "Default VPC: $VPC_ID"

      - name: Generate unique identifiers
        id: ids
        run: |
          TIMESTAMP=$(date +%Y%m%d%H%M%S)
          RANDOM_SUFFIX=$(head /dev/urandom | tr -dc a-z0-9 | head -c 6)
          TEMP_INSTANCE_ID="temp-${TIMESTAMP}-${RANDOM_SUFFIX}"

          # Generate random SSH port (10001-65535)
          SSH_PORT=$((10001 + RANDOM % 55535))
          echo "::add-mask::$SSH_PORT"
          echo "TIMESTAMP=$TIMESTAMP" >> $GITHUB_OUTPUT
          echo "TEMP_INSTANCE_ID=$TEMP_INSTANCE_ID" >> $GITHUB_OUTPUT
          echo "SSH_PORT=$SSH_PORT" >> $GITHUB_OUTPUT
          echo "Temporary Instance ID: $TEMP_INSTANCE_ID"
          echo "SSH Port: $SSH_PORT"

      - name: Setup persistent security group
        id: security-group
        run: |
          echo "Setting up persistent security group 'github-actions-persistent'..."

          SG_NAME="github-actions-persistent"
          SG_DESCRIPTION="Persistent security group for GitHub Actions with dynamic rules"

          # Check if security group already exists
          SG_ID=$(aws ec2 describe-security-groups \
            --filters "Name=group-name,Values=$SG_NAME" \
            --query 'SecurityGroups[0].GroupId' \
            --region "$AWS_REGION" \
            --output text || echo "")

          if [ "$SG_ID" = "None" ] || [ -z "$SG_ID" ]; then
            echo "Security group does not exist. Creating new one..."

            # Create security group
            SG_ID=$(aws ec2 create-security-group \
              --group-name "$SG_NAME" \
              --description "$SG_DESCRIPTION" \
              --vpc-id "${{ steps.vpc.outputs.VPC_ID }}" \
              --query 'GroupId' \
              --region "$AWS_REGION" \
              --output text)

            echo "Security Group created: $SG_ID"

            # Tag the security group
            aws ec2 create-tags \
              --resources "$SG_ID" \
              --tags \
                "Key=Name,Value=$SG_NAME" \
                "Key=ManagedBy,Value=GitHubActions" \
                "Key=Purpose,Value=PersistentDynamicRules" \
                "Key=CreatedAt,Value=$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
              --region "$AWS_REGION"

            echo "Security Group tagged successfully"
          else
            echo "Using existing security group: $SG_ID"
          fi

          echo "SG_ID=$SG_ID" >> $GITHUB_OUTPUT

          # Add only custom SSH port (no port 22)
          echo "Adding ingress rule for runner IP on port ${{ steps.ids.outputs.SSH_PORT }}"
          aws ec2 authorize-security-group-ingress \
            --group-id "$SG_ID" \
            --ip-permissions \
              "IpProtocol=tcp,FromPort=${{ steps.ids.outputs.SSH_PORT }},ToPort=${{ steps.ids.outputs.SSH_PORT }},IpRanges=[{CidrIp=${{ steps.runner-ip.outputs.RUNNER_IP }}/32,Description='GHA Run ${{ github.run_id }} Port ${{ steps.ids.outputs.SSH_PORT }}'}]" \
            --region "$AWS_REGION" 2>&1 || echo "Note: Rule may already exist"

          echo "RUNNER_IP_CIDR=${{ steps.runner-ip.outputs.RUNNER_IP }}/32" >> $GITHUB_OUTPUT
          echo "SSH_PORT=${{ steps.ids.outputs.SSH_PORT }}" >> $GITHUB_OUTPUT
          echo "SSH ingress rule added successfully (custom port only)"

          RULE_COUNT=$(aws ec2 describe-security-groups \
            --group-ids "$SG_ID" \
            --query 'length(SecurityGroups[0].IpPermissions)' \
            --region "$AWS_REGION" \
            --output text)
          echo "Security group has $RULE_COUNT active ingress rule(s)"

      - name: Retrieve SSH key from Parameter Store
        id: keypair
        run: |
          echo "Retrieving SSH private key from AWS Systems Manager..."
          KEY_NAME="${{ secrets.AWS_KEY_PAIR_NAME }}"

          # Retrieve the SSH private key from Parameter Store
          aws ssm get-parameter \
            --name "/github-actions/ec2/ssh-key" \
            --with-decryption \
            --query 'Parameter.Value' \
            --region "$AWS_REGION" \
            --output text > /tmp/github-actions-ec2.pem

          chmod 600 /tmp/github-actions-ec2.pem
          echo "SSH key retrieved successfully"
          echo "KEY_PATH=/tmp/github-actions-ec2.pem" >> $GITHUB_OUTPUT

      - name: Create user data script
        run: |
          echo '#!/bin/bash' > /tmp/user-data.sh
          echo 'set -x' >> /tmp/user-data.sh
          echo 'exec > >(tee /var/log/user-data.log) 2>&1' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo 'echo "=== User Data Script Started ==="' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo '# Wait for system to be ready' >> /tmp/user-data.sh
          echo 'sleep 5' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo '# Create SSH privilege separation directory' >> /tmp/user-data.sh
          echo 'echo "Creating /run/sshd directory"' >> /tmp/user-data.sh
          echo 'mkdir -p /run/sshd' >> /tmp/user-data.sh
          echo 'chmod 0755 /run/sshd' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo '# Configure custom SSH port' >> /tmp/user-data.sh
          echo 'echo "Configuring SSH port to '"${{ steps.ids.outputs.SSH_PORT }}"'"' >> /tmp/user-data.sh
          echo 'perl -pi -e "s/^#?Port 22$/Port '"${{ steps.ids.outputs.SSH_PORT }}"'/" /etc/ssh/sshd_config' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo '# Ensure Port directive exists' >> /tmp/user-data.sh
          echo 'if ! grep -q "^Port '"${{ steps.ids.outputs.SSH_PORT }}"'" /etc/ssh/sshd_config; then' >> /tmp/user-data.sh
          echo '  echo "Port '"${{ steps.ids.outputs.SSH_PORT }}"'" >> /etc/ssh/sshd_config' >> /tmp/user-data.sh
          echo 'fi' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo 'echo "SSH config after modification:"' >> /tmp/user-data.sh
          echo 'grep "^Port" /etc/ssh/sshd_config' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo '# Disable systemd socket activation' >> /tmp/user-data.sh
          echo 'echo "Disabling socket activation"' >> /tmp/user-data.sh
          echo 'systemctl stop ssh.socket' >> /tmp/user-data.sh
          echo 'systemctl disable ssh.socket' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo '# Test SSH configuration' >> /tmp/user-data.sh
          echo 'echo "Testing SSH configuration"' >> /tmp/user-data.sh
          echo 'sshd -t' >> /tmp/user-data.sh
          echo 'if [ $? -eq 0 ]; then' >> /tmp/user-data.sh
          echo '  echo "SSH config valid, restarting SSH service"' >> /tmp/user-data.sh
          echo '  systemctl restart ssh.service' >> /tmp/user-data.sh
          echo '  sleep 2' >> /tmp/user-data.sh
          echo '  systemctl status ssh.service' >> /tmp/user-data.sh
          echo '  echo "Checking listening ports:"' >> /tmp/user-data.sh
          echo '  ss -tlnp | grep sshd || netstat -tlnp | grep sshd' >> /tmp/user-data.sh
          echo '  echo "SSH reconfiguration successful"' >> /tmp/user-data.sh
          echo 'else' >> /tmp/user-data.sh
          echo '  echo "ERROR: SSH config invalid"' >> /tmp/user-data.sh
          echo '  exit 1' >> /tmp/user-data.sh
          echo 'fi' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo '# Install Rust (needed for cargo build)' >> /tmp/user-data.sh
          echo 'curl --proto '"'"'=https'"'"' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y' >> /tmp/user-data.sh
          echo 'source "$HOME/.cargo/env"' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo '# Verify nvidia-smi is available' >> /tmp/user-data.sh
          echo 'if command -v nvidia-smi &> /dev/null; then' >> /tmp/user-data.sh
          echo '    echo "NVIDIA drivers confirmed"' >> /tmp/user-data.sh
          echo '    nvidia-smi' >> /tmp/user-data.sh
          echo 'else' >> /tmp/user-data.sh
          echo '    echo "Warning: nvidia-smi not found"' >> /tmp/user-data.sh
          echo 'fi' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo '# Create workspace directory' >> /tmp/user-data.sh
          echo 'mkdir -p ${{ env.WORKDIR }}/workspace' >> /tmp/user-data.sh
          echo 'chown -R ${{ env.USER }}:${{ env.USER }} ${{ env.WORKDIR }}/workspace' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo 'nvidia-smi | tee /tmp/nvidia-smi-output.txt' >> /tmp/user-data.sh
          echo 'touch /tmp/setup-complete' >> /tmp/user-data.sh
          echo 'echo "=== User Data Script Complete ==="' >> /tmp/user-data.sh

      - name: Launch EC2 instance
        id: instance
        run: |
          echo "Launching EC2 instance with SSH configured on port ${{ steps.ids.outputs.SSH_PORT }}..."

          # Base64 encode for AWS
          USER_DATA=$(base64 -w 0 /tmp/user-data.sh)

          INSTANCE_ID=$(aws ec2 run-instances \
            --image-id "${{ steps.ami.outputs.AMI_ID }}" \
            --instance-type "$INSTANCE_TYPE" \
            --key-name "${{ secrets.AWS_KEY_PAIR_NAME }}" \
            --security-group-ids "${{ steps.security-group.outputs.SG_ID }}" \
            --user-data "$USER_DATA" \
            --block-device-mappings "DeviceName=/dev/sda1,Ebs={VolumeSize=256,VolumeType=gp3,DeleteOnTermination=true}" \
            --tag-specifications \
              "ResourceType=instance,Tags=[\
                {Key=Name,Value=gpu-runner-batch-1-${{ steps.ids.outputs.TIMESTAMP }}},\
                {Key=ManagedBy,Value=GitHubActions},\
                {Key=Purpose,Value=GPURunner},\
                {Key=Workflow,Value=${{ github.workflow }}},\
                {Key=RunId,Value=${{ github.run_id }}},\
                {Key=Branch,Value=${{ env.BRANCH }}},\
                {Key=Batch,Value=1}\
              ]" \
              "ResourceType=volume,Tags=[\
                {Key=Name,Value=gpu-runner-batch-1-${{ steps.ids.outputs.TIMESTAMP }}-volume},\
                {Key=ManagedBy,Value=GitHubActions},\
                {Key=Purpose,Value=GPURunner},\
                {Key=Workflow,Value=${{ github.workflow }}},\
                {Key=Batch,Value=1}\
              ]" \
            --instance-initiated-shutdown-behavior terminate \
            --query 'Instances[0].InstanceId' \
            --region "$AWS_REGION" \
            --output text)

          echo "INSTANCE_ID=$INSTANCE_ID" >> $GITHUB_OUTPUT
          echo "Instance launched: $INSTANCE_ID"

      - name: Wait for instance to be running
        run: |
          echo "Waiting for instance to be running..."
          aws ec2 wait instance-running \
            --instance-ids "${{ steps.instance.outputs.INSTANCE_ID }}" \
            --region "$AWS_REGION"

          PUBLIC_IP=$(aws ec2 describe-instances \
            --instance-ids "${{ steps.instance.outputs.INSTANCE_ID }}" \
            --query 'Reservations[0].Instances[0].PublicIpAddress' \
            --region "$AWS_REGION" \
            --output text)

          echo "::add-mask::$PUBLIC_IP"
          echo "PUBLIC_IP=$PUBLIC_IP" >> $GITHUB_ENV
          echo "Instance is running at: $PUBLIC_IP"

      - name: Wait for cloud-init and SSH on custom port
        run: |
          echo "Waiting for cloud-init to complete and SSH to be available on port ${{ steps.ids.outputs.SSH_PORT }}..."

          # Wait longer initially to allow cloud-init to run
          echo "Waiting 60 seconds for cloud-init to start..."
          sleep 60

          MAX_ATTEMPTS=40
          ATTEMPT=0

          while [ $ATTEMPT -lt $MAX_ATTEMPTS ]; do
            if ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o ConnectTimeout=5 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
              -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} "echo 'SSH ready on custom port'" 2>/dev/null; then
              echo "SSH connection established on port ${{ steps.ids.outputs.SSH_PORT }}"
              break
            else
              ATTEMPT=$((ATTEMPT + 1))
              if [ $ATTEMPT -eq $MAX_ATTEMPTS ]; then
                echo "Failed to establish SSH connection on port ${{ steps.ids.outputs.SSH_PORT }} after $MAX_ATTEMPTS attempts"
                echo "Attempting to fetch console output for debugging..."
                aws ec2 get-console-output \
                  --instance-id "${{ steps.instance.outputs.INSTANCE_ID }}" \
                  --region "$AWS_REGION" \
                  --output text || echo "Could not fetch console output"
                exit 1
              fi
              echo "Attempt $ATTEMPT/$MAX_ATTEMPTS failed, retrying in 10 seconds..."
              sleep 10
            fi
          done

      - name: Wait for instance setup
        run: |
          echo "Waiting for instance setup to complete..."
          MAX_WAIT=300
          ELAPSED=0

          while [ $ELAPSED -lt $MAX_WAIT ]; do
            if ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
              -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} \
              "test -f /tmp/setup-complete" 2>/dev/null; then
              echo "Instance setup completed"
              break
            else
              sleep 10
              ELAPSED=$((ELAPSED + 10))
              if [ $ELAPSED -ge $MAX_WAIT ]; then
                echo "Setup timeout, continuing anyway..."
                break
              fi
            fi
          done

      - name: Create archive of repository
        run: |
          echo "Creating repository archive..."
          git archive --format=tar.gz --output=/tmp/repo.tar.gz HEAD

      - name: Transfer repository to instance
        run: |
          echo "Transferring repository to instance..."
          scp -P ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" \
            /tmp/repo.tar.gz ${{ env.USER }}@${{ env.PUBLIC_IP }}:${{ env.WORKDIR }}/

          echo "Extracting repository on instance..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} \
            "cd ${{ env.WORKDIR }} && tar -xzf repo.tar.gz && rm repo.tar.gz"

      - name: Setup Python environment and run warmup
        run: |
          echo "Setting up Python environment and running warmup.py..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} << 'ENDSSH'
          set -e
          cd ${{ env.WORKDIR }}

          # Run warmup.py
          echo "Running warmup.py..."
          python3 warmup.py --skip-confirmation

          echo "Warmup completed"
          ENDSSH

      - name: Build Rust project
        run: |
          echo "Building Rust project with cargo..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} << 'ENDSSH'
          set -e
          cd ${{ env.WORKDIR }}

          # Setup Rust environment
          source "$HOME/.cargo/env"

          # Build the project
          echo "Running cargo build --release..."
          cargo build --release

          echo "Cargo build completed"
          ENDSSH

      - name: Setup CI directory
        run: |
          echo "Setting up CI directory..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} \
            "mkdir -p /tmp/ci"

      - name: Run belt
        run: |
          echo "Running belt..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} << 'ENDSSH'
          set -e
          cd ${{ env.WORKDIR }}

          # Activate Python environment
          source ~/.local/share/ppf-cts/venv/bin/activate

          # Convert notebook to Python script
          jupyter nbconvert --to python "examples/belt.ipynb" --output "/tmp/belt_base.py"

          # Create the runnable script with proper imports
          cat > /tmp/belt.py << 'PYEOF'
          import sys
          import os

          # Add the repository root to Python path so frontend can be imported
          sys.path.insert(0, '${{ env.WORKDIR }}')
          sys.path.insert(0, '${{ env.WORKDIR }}/frontend')

          # Set environment variables if needed
          os.environ['PYTHONPATH'] = '${{ env.WORKDIR }}:${{ env.WORKDIR }}/frontend:' + os.environ.get('PYTHONPATH', '')
          PYEOF

          # Append the converted notebook content
          cat "/tmp/belt_base.py" >> /tmp/belt.py

          # Run the example
          echo "belt" > frontend/.CI
          python3 /tmp/belt.py 2>&1 | tee /tmp/ci/belt.log
          ENDSSH

      - name: Run cards
        run: |
          echo "Running cards..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} << 'ENDSSH'
          set -e
          cd ${{ env.WORKDIR }}

          # Activate Python environment
          source ~/.local/share/ppf-cts/venv/bin/activate

          # Convert notebook to Python script
          jupyter nbconvert --to python "examples/cards.ipynb" --output "/tmp/cards_base.py"

          # Create the runnable script with proper imports
          cat > /tmp/cards.py << 'PYEOF'
          import sys
          import os

          # Add the repository root to Python path so frontend can be imported
          sys.path.insert(0, '${{ env.WORKDIR }}')
          sys.path.insert(0, '${{ env.WORKDIR }}/frontend')

          # Set environment variables if needed
          os.environ['PYTHONPATH'] = '${{ env.WORKDIR }}:${{ env.WORKDIR }}/frontend:' + os.environ.get('PYTHONPATH', '')
          PYEOF

          # Append the converted notebook content
          cat "/tmp/cards_base.py" >> /tmp/cards.py

          # Run the example
          echo "cards" > frontend/.CI
          python3 /tmp/cards.py 2>&1 | tee /tmp/ci/cards.log
          ENDSSH

      - name: Run codim
        run: |
          echo "Running codim..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} << 'ENDSSH'
          set -e
          cd ${{ env.WORKDIR }}

          # Activate Python environment
          source ~/.local/share/ppf-cts/venv/bin/activate

          # Convert notebook to Python script
          jupyter nbconvert --to python "examples/codim.ipynb" --output "/tmp/codim_base.py"

          # Create the runnable script with proper imports
          cat > /tmp/codim.py << 'PYEOF'
          import sys
          import os

          # Add the repository root to Python path so frontend can be imported
          sys.path.insert(0, '${{ env.WORKDIR }}')
          sys.path.insert(0, '${{ env.WORKDIR }}/frontend')

          # Set environment variables if needed
          os.environ['PYTHONPATH'] = '${{ env.WORKDIR }}:${{ env.WORKDIR }}/frontend:' + os.environ.get('PYTHONPATH', '')
          PYEOF

          # Append the converted notebook content
          cat "/tmp/codim_base.py" >> /tmp/codim.py

          # Run the example
          echo "codim" > frontend/.CI
          python3 /tmp/codim.py 2>&1 | tee /tmp/ci/codim.log
          ENDSSH

      - name: Run curtain
        run: |
          echo "Running curtain..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} << 'ENDSSH'
          set -e
          cd ${{ env.WORKDIR }}

          # Activate Python environment
          source ~/.local/share/ppf-cts/venv/bin/activate

          # Convert notebook to Python script
          jupyter nbconvert --to python "examples/curtain.ipynb" --output "/tmp/curtain_base.py"

          # Create the runnable script with proper imports
          cat > /tmp/curtain.py << 'PYEOF'
          import sys
          import os

          # Add the repository root to Python path so frontend can be imported
          sys.path.insert(0, '${{ env.WORKDIR }}')
          sys.path.insert(0, '${{ env.WORKDIR }}/frontend')

          # Set environment variables if needed
          os.environ['PYTHONPATH'] = '${{ env.WORKDIR }}:${{ env.WORKDIR }}/frontend:' + os.environ.get('PYTHONPATH', '')
          PYEOF

          # Append the converted notebook content
          cat "/tmp/curtain_base.py" >> /tmp/curtain.py

          # Run the example
          echo "curtain" > frontend/.CI
          python3 /tmp/curtain.py 2>&1 | tee /tmp/ci/curtain.log
          ENDSSH

      - name: Run domino
        run: |
          echo "Running domino..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} << 'ENDSSH'
          set -e
          cd ${{ env.WORKDIR }}

          # Activate Python environment
          source ~/.local/share/ppf-cts/venv/bin/activate

          # Convert notebook to Python script
          jupyter nbconvert --to python "examples/domino.ipynb" --output "/tmp/domino_base.py"

          # Create the runnable script with proper imports
          cat > /tmp/domino.py << 'PYEOF'
          import sys
          import os

          # Add the repository root to Python path so frontend can be imported
          sys.path.insert(0, '${{ env.WORKDIR }}')
          sys.path.insert(0, '${{ env.WORKDIR }}/frontend')

          # Set environment variables if needed
          os.environ['PYTHONPATH'] = '${{ env.WORKDIR }}:${{ env.WORKDIR }}/frontend:' + os.environ.get('PYTHONPATH', '')
          PYEOF

          # Append the converted notebook content
          cat "/tmp/domino_base.py" >> /tmp/domino.py

          # Run the example
          echo "domino" > frontend/.CI
          python3 /tmp/domino.py 2>&1 | tee /tmp/ci/domino.log
          ENDSSH



      - name: Collect results
        if: success() || failure()
        run: |
          echo "Collecting results from all runs..."
          mkdir -p ci
          rsync -avz --exclude='*.bin' --exclude='*.pickle' -e "ssh -p ${{ steps.ids.outputs.SSH_PORT }} -i ${{ steps.keypair.outputs.KEY_PATH }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null" \
            ${{ env.USER }}@${{ env.PUBLIC_IP }}:/tmp/ci/ ./ci/
          echo "## Collected Files:"
          ls -la ci/
          echo "## Run Summary:"
          for log in ci/*.log; do
            if [ -f "$log" ]; then
              echo "Found: $log"
            fi
          done

      - name: Upload artifact
        if: success() || failure()
        uses: actions/upload-artifact@v4
        with:
          name: ci-batch-1
          path: ci
          retention-days: 3

      - name: GPU information
        if: success() || failure()
        run: |
          echo "Getting GPU information..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} \
            "nvidia-smi" || echo "Failed to get GPU info"

      - name: Re-authenticate for cleanup
        if: always()
        continue-on-error: true
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Cleanup - Terminate Instance
        if: always()
        continue-on-error: true
        run: |
          if [ -n "${{ steps.instance.outputs.INSTANCE_ID }}" ]; then
            echo "Initiating instance termination: ${{ steps.instance.outputs.INSTANCE_ID }}"
            aws ec2 terminate-instances \
              --instance-ids "${{ steps.instance.outputs.INSTANCE_ID }}" \
              --region "$AWS_REGION" || true
            echo "Termination initiated. Instance will terminate in the background."
          else
            echo "No instance to terminate"
          fi

      - name: Cleanup - Remove Ingress Rules
        if: always()
        continue-on-error: true
        run: |
          if [ -n "${{ steps.security-group.outputs.SG_ID }}" ] && [ -n "${{ steps.security-group.outputs.RUNNER_IP_CIDR }}" ]; then
            echo "Removing ingress rules from security group ${{ steps.security-group.outputs.SG_ID }}"

            # Remove custom port rule
            if [ -n "${{ steps.security-group.outputs.SSH_PORT }}" ]; then
              echo "Removing port ${{ steps.security-group.outputs.SSH_PORT }} rule..."
              aws ec2 revoke-security-group-ingress \
                --group-id "${{ steps.security-group.outputs.SG_ID }}" \
                --ip-permissions \
                  "IpProtocol=tcp,FromPort=${{ steps.security-group.outputs.SSH_PORT }},ToPort=${{ steps.security-group.outputs.SSH_PORT }},IpRanges=[{CidrIp=${{ steps.security-group.outputs.RUNNER_IP_CIDR }}}]" \
                --region "$AWS_REGION" 2>&1 || echo "Note: Custom port rule may have already been removed"
            fi

            echo "Ingress rule removed successfully"
            echo "Security group ${{ steps.security-group.outputs.SG_ID }} remains for future use"
          else
            echo "No ingress rules to remove"
          fi

      - name: Cleanup - Remove Local SSH Key
        if: always()
        continue-on-error: true
        run: |
          if [ -n "${{ steps.keypair.outputs.KEY_PATH }}" ] && [ -f "${{ steps.keypair.outputs.KEY_PATH }}" ]; then
            rm -f "${{ steps.keypair.outputs.KEY_PATH }}"
            echo "Local SSH key file removed"
          fi

      - name: Summary
        if: always()
        run: |
          echo "## Workflow Summary - Batch 1"
          echo "- Region: $AWS_REGION"
          echo "- Instance Type: $INSTANCE_TYPE"
          echo "- Branch: $BRANCH"
          echo "- Examples: $EXAMPLES"
          echo "- Instance ID: ${{ steps.instance.outputs.INSTANCE_ID || 'Not launched' }}"
          echo "- Run Status: ${{ steps.run-examples.outcome || 'Not run' }}"

  run-batch-2:
    name: Run Batch 2
    runs-on: ubuntu-latest
    permissions:
      id-token: write
      contents: read

    env:
      AWS_REGION: ${{ github.event.inputs.region }}
      INSTANCE_TYPE: ${{ github.event.inputs.instance_type }}
      BRANCH: ${{ github.ref_name }}
      EXAMPLES: "drape fishingknot fitting friction"
      WORKDIR: /home/ubuntu
      USER: ubuntu

    steps:
      - name: Show input parameters
        run: |
          echo "## Input Parameters - Batch 2"
          echo "Branch: ${{ github.ref_name }}"
          echo "Instance Type: ${{ github.event.inputs.instance_type }}"
          echo "Region: ${{ github.event.inputs.region }}"
          echo "Examples: drape fishingknot fitting friction"

      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Verify AWS authentication
        run: |
          echo "Testing AWS authentication..."
          aws sts get-caller-identity
          echo "AWS Region: $AWS_REGION"
          echo "Instance Type: $INSTANCE_TYPE"
          echo "Branch: $BRANCH"
          echo "Examples: $EXAMPLES"

      - name: Get GitHub Actions runner public IP
        id: runner-ip
        run: |
          echo "Fetching GitHub Actions runner public IP..."
          RUNNER_IP=$(curl -s --max-time 10 https://checkip.amazonaws.com | tr -d '\n')
          if [ -z "$RUNNER_IP" ]; then
            echo "ERROR: Failed to get IP from checkip.amazonaws.com"
            exit 1
          fi
          echo "::add-mask::$RUNNER_IP"
          echo "RUNNER_IP=$RUNNER_IP" >> $GITHUB_OUTPUT
          echo "GitHub Actions Runner IP: $RUNNER_IP"

      - name: Find Deep Learning AMI
        id: ami
        run: |
          echo "Finding latest Deep Learning AMI with GPU support..."
          AMI_ID=$(aws ec2 describe-images \
            --owners amazon \
            --filters \
              "Name=name,Values=Deep Learning Base OSS Nvidia Driver GPU AMI (Ubuntu 24.04)*" \
              "Name=state,Values=available" \
              "Name=architecture,Values=x86_64" \
            --query 'sort_by(Images, &CreationDate)[-1].ImageId' \
            --region "$AWS_REGION" \
            --output text)

          if [ "$AMI_ID" = "None" ] || [ -z "$AMI_ID" ]; then
            echo "ERROR: Deep Learning AMI not found in region $AWS_REGION"
            echo "This workflow requires the Deep Learning AMI with pre-installed NVIDIA drivers"
            echo "Please check if the AMI is available in your selected region"
            exit 1
          fi

          echo "AMI_ID=$AMI_ID" >> $GITHUB_OUTPUT
          echo "Found AMI: $AMI_ID"

      - name: Get default VPC ID
        id: vpc
        run: |
          echo "Getting default VPC ID..."
          VPC_ID=$(aws ec2 describe-vpcs \
            --filters "Name=isDefault,Values=true" \
            --query 'Vpcs[0].VpcId' \
            --region "$AWS_REGION" \
            --output text)

          if [ "$VPC_ID" = "None" ] || [ -z "$VPC_ID" ]; then
            echo "ERROR: Default VPC not found in region $AWS_REGION"
            exit 1
          fi

          echo "VPC_ID=$VPC_ID" >> $GITHUB_OUTPUT
          echo "Default VPC: $VPC_ID"

      - name: Generate unique identifiers
        id: ids
        run: |
          TIMESTAMP=$(date +%Y%m%d%H%M%S)
          RANDOM_SUFFIX=$(head /dev/urandom | tr -dc a-z0-9 | head -c 6)
          TEMP_INSTANCE_ID="temp-${TIMESTAMP}-${RANDOM_SUFFIX}"

          # Generate random SSH port (10001-65535)
          SSH_PORT=$((10001 + RANDOM % 55535))
          echo "::add-mask::$SSH_PORT"
          echo "TIMESTAMP=$TIMESTAMP" >> $GITHUB_OUTPUT
          echo "TEMP_INSTANCE_ID=$TEMP_INSTANCE_ID" >> $GITHUB_OUTPUT
          echo "SSH_PORT=$SSH_PORT" >> $GITHUB_OUTPUT
          echo "Temporary Instance ID: $TEMP_INSTANCE_ID"
          echo "SSH Port: $SSH_PORT"

      - name: Setup persistent security group
        id: security-group
        run: |
          echo "Setting up persistent security group 'github-actions-persistent'..."

          SG_NAME="github-actions-persistent"
          SG_DESCRIPTION="Persistent security group for GitHub Actions with dynamic rules"

          # Check if security group already exists
          SG_ID=$(aws ec2 describe-security-groups \
            --filters "Name=group-name,Values=$SG_NAME" \
            --query 'SecurityGroups[0].GroupId' \
            --region "$AWS_REGION" \
            --output text || echo "")

          if [ "$SG_ID" = "None" ] || [ -z "$SG_ID" ]; then
            echo "Security group does not exist. Creating new one..."

            # Create security group
            SG_ID=$(aws ec2 create-security-group \
              --group-name "$SG_NAME" \
              --description "$SG_DESCRIPTION" \
              --vpc-id "${{ steps.vpc.outputs.VPC_ID }}" \
              --query 'GroupId' \
              --region "$AWS_REGION" \
              --output text)

            echo "Security Group created: $SG_ID"

            # Tag the security group
            aws ec2 create-tags \
              --resources "$SG_ID" \
              --tags \
                "Key=Name,Value=$SG_NAME" \
                "Key=ManagedBy,Value=GitHubActions" \
                "Key=Purpose,Value=PersistentDynamicRules" \
                "Key=CreatedAt,Value=$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
              --region "$AWS_REGION"

            echo "Security Group tagged successfully"
          else
            echo "Using existing security group: $SG_ID"
          fi

          echo "SG_ID=$SG_ID" >> $GITHUB_OUTPUT

          # Add only custom SSH port (no port 22)
          echo "Adding ingress rule for runner IP on port ${{ steps.ids.outputs.SSH_PORT }}"
          aws ec2 authorize-security-group-ingress \
            --group-id "$SG_ID" \
            --ip-permissions \
              "IpProtocol=tcp,FromPort=${{ steps.ids.outputs.SSH_PORT }},ToPort=${{ steps.ids.outputs.SSH_PORT }},IpRanges=[{CidrIp=${{ steps.runner-ip.outputs.RUNNER_IP }}/32,Description='GHA Run ${{ github.run_id }} Port ${{ steps.ids.outputs.SSH_PORT }}'}]" \
            --region "$AWS_REGION" 2>&1 || echo "Note: Rule may already exist"

          echo "RUNNER_IP_CIDR=${{ steps.runner-ip.outputs.RUNNER_IP }}/32" >> $GITHUB_OUTPUT
          echo "SSH_PORT=${{ steps.ids.outputs.SSH_PORT }}" >> $GITHUB_OUTPUT
          echo "SSH ingress rule added successfully (custom port only)"

          RULE_COUNT=$(aws ec2 describe-security-groups \
            --group-ids "$SG_ID" \
            --query 'length(SecurityGroups[0].IpPermissions)' \
            --region "$AWS_REGION" \
            --output text)
          echo "Security group has $RULE_COUNT active ingress rule(s)"

      - name: Retrieve SSH key from Parameter Store
        id: keypair
        run: |
          echo "Retrieving SSH private key from AWS Systems Manager..."
          KEY_NAME="${{ secrets.AWS_KEY_PAIR_NAME }}"

          # Retrieve the SSH private key from Parameter Store
          aws ssm get-parameter \
            --name "/github-actions/ec2/ssh-key" \
            --with-decryption \
            --query 'Parameter.Value' \
            --region "$AWS_REGION" \
            --output text > /tmp/github-actions-ec2.pem

          chmod 600 /tmp/github-actions-ec2.pem
          echo "SSH key retrieved successfully"
          echo "KEY_PATH=/tmp/github-actions-ec2.pem" >> $GITHUB_OUTPUT

      - name: Create user data script
        run: |
          echo '#!/bin/bash' > /tmp/user-data.sh
          echo 'set -x' >> /tmp/user-data.sh
          echo 'exec > >(tee /var/log/user-data.log) 2>&1' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo 'echo "=== User Data Script Started ==="' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo '# Wait for system to be ready' >> /tmp/user-data.sh
          echo 'sleep 5' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo '# Create SSH privilege separation directory' >> /tmp/user-data.sh
          echo 'echo "Creating /run/sshd directory"' >> /tmp/user-data.sh
          echo 'mkdir -p /run/sshd' >> /tmp/user-data.sh
          echo 'chmod 0755 /run/sshd' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo '# Configure custom SSH port' >> /tmp/user-data.sh
          echo 'echo "Configuring SSH port to '"${{ steps.ids.outputs.SSH_PORT }}"'"' >> /tmp/user-data.sh
          echo 'perl -pi -e "s/^#?Port 22$/Port '"${{ steps.ids.outputs.SSH_PORT }}"'/" /etc/ssh/sshd_config' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo '# Ensure Port directive exists' >> /tmp/user-data.sh
          echo 'if ! grep -q "^Port '"${{ steps.ids.outputs.SSH_PORT }}"'" /etc/ssh/sshd_config; then' >> /tmp/user-data.sh
          echo '  echo "Port '"${{ steps.ids.outputs.SSH_PORT }}"'" >> /etc/ssh/sshd_config' >> /tmp/user-data.sh
          echo 'fi' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo 'echo "SSH config after modification:"' >> /tmp/user-data.sh
          echo 'grep "^Port" /etc/ssh/sshd_config' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo '# Disable systemd socket activation' >> /tmp/user-data.sh
          echo 'echo "Disabling socket activation"' >> /tmp/user-data.sh
          echo 'systemctl stop ssh.socket' >> /tmp/user-data.sh
          echo 'systemctl disable ssh.socket' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo '# Test SSH configuration' >> /tmp/user-data.sh
          echo 'echo "Testing SSH configuration"' >> /tmp/user-data.sh
          echo 'sshd -t' >> /tmp/user-data.sh
          echo 'if [ $? -eq 0 ]; then' >> /tmp/user-data.sh
          echo '  echo "SSH config valid, restarting SSH service"' >> /tmp/user-data.sh
          echo '  systemctl restart ssh.service' >> /tmp/user-data.sh
          echo '  sleep 2' >> /tmp/user-data.sh
          echo '  systemctl status ssh.service' >> /tmp/user-data.sh
          echo '  echo "Checking listening ports:"' >> /tmp/user-data.sh
          echo '  ss -tlnp | grep sshd || netstat -tlnp | grep sshd' >> /tmp/user-data.sh
          echo '  echo "SSH reconfiguration successful"' >> /tmp/user-data.sh
          echo 'else' >> /tmp/user-data.sh
          echo '  echo "ERROR: SSH config invalid"' >> /tmp/user-data.sh
          echo '  exit 1' >> /tmp/user-data.sh
          echo 'fi' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo '# Install Rust (needed for cargo build)' >> /tmp/user-data.sh
          echo 'curl --proto '"'"'=https'"'"' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y' >> /tmp/user-data.sh
          echo 'source "$HOME/.cargo/env"' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo '# Verify nvidia-smi is available' >> /tmp/user-data.sh
          echo 'if command -v nvidia-smi &> /dev/null; then' >> /tmp/user-data.sh
          echo '    echo "NVIDIA drivers confirmed"' >> /tmp/user-data.sh
          echo '    nvidia-smi' >> /tmp/user-data.sh
          echo 'else' >> /tmp/user-data.sh
          echo '    echo "Warning: nvidia-smi not found"' >> /tmp/user-data.sh
          echo 'fi' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo '# Create workspace directory' >> /tmp/user-data.sh
          echo 'mkdir -p ${{ env.WORKDIR }}/workspace' >> /tmp/user-data.sh
          echo 'chown -R ${{ env.USER }}:${{ env.USER }} ${{ env.WORKDIR }}/workspace' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo 'nvidia-smi | tee /tmp/nvidia-smi-output.txt' >> /tmp/user-data.sh
          echo 'touch /tmp/setup-complete' >> /tmp/user-data.sh
          echo 'echo "=== User Data Script Complete ==="' >> /tmp/user-data.sh

      - name: Launch EC2 instance
        id: instance
        run: |
          echo "Launching EC2 instance with SSH configured on port ${{ steps.ids.outputs.SSH_PORT }}..."

          # Base64 encode for AWS
          USER_DATA=$(base64 -w 0 /tmp/user-data.sh)

          INSTANCE_ID=$(aws ec2 run-instances \
            --image-id "${{ steps.ami.outputs.AMI_ID }}" \
            --instance-type "$INSTANCE_TYPE" \
            --key-name "${{ secrets.AWS_KEY_PAIR_NAME }}" \
            --security-group-ids "${{ steps.security-group.outputs.SG_ID }}" \
            --user-data "$USER_DATA" \
            --block-device-mappings "DeviceName=/dev/sda1,Ebs={VolumeSize=256,VolumeType=gp3,DeleteOnTermination=true}" \
            --tag-specifications \
              "ResourceType=instance,Tags=[\
                {Key=Name,Value=gpu-runner-batch-2-${{ steps.ids.outputs.TIMESTAMP }}},\
                {Key=ManagedBy,Value=GitHubActions},\
                {Key=Purpose,Value=GPURunner},\
                {Key=Workflow,Value=${{ github.workflow }}},\
                {Key=RunId,Value=${{ github.run_id }}},\
                {Key=Branch,Value=${{ env.BRANCH }}},\
                {Key=Batch,Value=2}\
              ]" \
              "ResourceType=volume,Tags=[\
                {Key=Name,Value=gpu-runner-batch-2-${{ steps.ids.outputs.TIMESTAMP }}-volume},\
                {Key=ManagedBy,Value=GitHubActions},\
                {Key=Purpose,Value=GPURunner},\
                {Key=Workflow,Value=${{ github.workflow }}},\
                {Key=Batch,Value=2}\
              ]" \
            --instance-initiated-shutdown-behavior terminate \
            --query 'Instances[0].InstanceId' \
            --region "$AWS_REGION" \
            --output text)

          echo "INSTANCE_ID=$INSTANCE_ID" >> $GITHUB_OUTPUT
          echo "Instance launched: $INSTANCE_ID"

      - name: Wait for instance to be running
        run: |
          echo "Waiting for instance to be running..."
          aws ec2 wait instance-running \
            --instance-ids "${{ steps.instance.outputs.INSTANCE_ID }}" \
            --region "$AWS_REGION"

          PUBLIC_IP=$(aws ec2 describe-instances \
            --instance-ids "${{ steps.instance.outputs.INSTANCE_ID }}" \
            --query 'Reservations[0].Instances[0].PublicIpAddress' \
            --region "$AWS_REGION" \
            --output text)

          echo "::add-mask::$PUBLIC_IP"
          echo "PUBLIC_IP=$PUBLIC_IP" >> $GITHUB_ENV
          echo "Instance is running at: $PUBLIC_IP"

      - name: Wait for cloud-init and SSH on custom port
        run: |
          echo "Waiting for cloud-init to complete and SSH to be available on port ${{ steps.ids.outputs.SSH_PORT }}..."

          # Wait longer initially to allow cloud-init to run
          echo "Waiting 60 seconds for cloud-init to start..."
          sleep 60

          MAX_ATTEMPTS=40
          ATTEMPT=0

          while [ $ATTEMPT -lt $MAX_ATTEMPTS ]; do
            if ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o ConnectTimeout=5 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
              -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} "echo 'SSH ready on custom port'" 2>/dev/null; then
              echo "SSH connection established on port ${{ steps.ids.outputs.SSH_PORT }}"
              break
            else
              ATTEMPT=$((ATTEMPT + 1))
              if [ $ATTEMPT -eq $MAX_ATTEMPTS ]; then
                echo "Failed to establish SSH connection on port ${{ steps.ids.outputs.SSH_PORT }} after $MAX_ATTEMPTS attempts"
                echo "Attempting to fetch console output for debugging..."
                aws ec2 get-console-output \
                  --instance-id "${{ steps.instance.outputs.INSTANCE_ID }}" \
                  --region "$AWS_REGION" \
                  --output text || echo "Could not fetch console output"
                exit 1
              fi
              echo "Attempt $ATTEMPT/$MAX_ATTEMPTS failed, retrying in 10 seconds..."
              sleep 10
            fi
          done

      - name: Wait for instance setup
        run: |
          echo "Waiting for instance setup to complete..."
          MAX_WAIT=300
          ELAPSED=0

          while [ $ELAPSED -lt $MAX_WAIT ]; do
            if ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
              -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} \
              "test -f /tmp/setup-complete" 2>/dev/null; then
              echo "Instance setup completed"
              break
            else
              sleep 10
              ELAPSED=$((ELAPSED + 10))
              if [ $ELAPSED -ge $MAX_WAIT ]; then
                echo "Setup timeout, continuing anyway..."
                break
              fi
            fi
          done

      - name: Create archive of repository
        run: |
          echo "Creating repository archive..."
          git archive --format=tar.gz --output=/tmp/repo.tar.gz HEAD

      - name: Transfer repository to instance
        run: |
          echo "Transferring repository to instance..."
          scp -P ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" \
            /tmp/repo.tar.gz ${{ env.USER }}@${{ env.PUBLIC_IP }}:${{ env.WORKDIR }}/

          echo "Extracting repository on instance..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} \
            "cd ${{ env.WORKDIR }} && tar -xzf repo.tar.gz && rm repo.tar.gz"

      - name: Setup Python environment and run warmup
        run: |
          echo "Setting up Python environment and running warmup.py..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} << 'ENDSSH'
          set -e
          cd ${{ env.WORKDIR }}

          # Run warmup.py
          echo "Running warmup.py..."
          python3 warmup.py --skip-confirmation

          echo "Warmup completed"
          ENDSSH

      - name: Build Rust project
        run: |
          echo "Building Rust project with cargo..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} << 'ENDSSH'
          set -e
          cd ${{ env.WORKDIR }}

          # Setup Rust environment
          source "$HOME/.cargo/env"

          # Build the project
          echo "Running cargo build --release..."
          cargo build --release

          echo "Cargo build completed"
          ENDSSH

      - name: Setup CI directory
        run: |
          echo "Setting up CI directory..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} \
            "mkdir -p /tmp/ci"

      - name: Run drape
        run: |
          echo "Running drape..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} << 'ENDSSH'
          set -e
          cd ${{ env.WORKDIR }}

          # Activate Python environment
          source ~/.local/share/ppf-cts/venv/bin/activate

          # Convert notebook to Python script
          jupyter nbconvert --to python "examples/drape.ipynb" --output "/tmp/drape_base.py"

          # Create the runnable script with proper imports
          cat > /tmp/drape.py << 'PYEOF'
          import sys
          import os

          # Add the repository root to Python path so frontend can be imported
          sys.path.insert(0, '${{ env.WORKDIR }}')
          sys.path.insert(0, '${{ env.WORKDIR }}/frontend')

          # Set environment variables if needed
          os.environ['PYTHONPATH'] = '${{ env.WORKDIR }}:${{ env.WORKDIR }}/frontend:' + os.environ.get('PYTHONPATH', '')
          PYEOF

          # Append the converted notebook content
          cat "/tmp/drape_base.py" >> /tmp/drape.py

          # Run the example
          echo "drape" > frontend/.CI
          python3 /tmp/drape.py 2>&1 | tee /tmp/ci/drape.log
          ENDSSH

      - name: Run fishingknot
        run: |
          echo "Running fishingknot..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} << 'ENDSSH'
          set -e
          cd ${{ env.WORKDIR }}

          # Activate Python environment
          source ~/.local/share/ppf-cts/venv/bin/activate

          # Convert notebook to Python script
          jupyter nbconvert --to python "examples/fishingknot.ipynb" --output "/tmp/fishingknot_base.py"

          # Create the runnable script with proper imports
          cat > /tmp/fishingknot.py << 'PYEOF'
          import sys
          import os

          # Add the repository root to Python path so frontend can be imported
          sys.path.insert(0, '${{ env.WORKDIR }}')
          sys.path.insert(0, '${{ env.WORKDIR }}/frontend')

          # Set environment variables if needed
          os.environ['PYTHONPATH'] = '${{ env.WORKDIR }}:${{ env.WORKDIR }}/frontend:' + os.environ.get('PYTHONPATH', '')
          PYEOF

          # Append the converted notebook content
          cat "/tmp/fishingknot_base.py" >> /tmp/fishingknot.py

          # Run the example
          echo "fishingknot" > frontend/.CI
          python3 /tmp/fishingknot.py 2>&1 | tee /tmp/ci/fishingknot.log
          ENDSSH

      - name: Run fitting
        run: |
          echo "Running fitting..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} << 'ENDSSH'
          set -e
          cd ${{ env.WORKDIR }}

          # Activate Python environment
          source ~/.local/share/ppf-cts/venv/bin/activate

          # Convert notebook to Python script
          jupyter nbconvert --to python "examples/fitting.ipynb" --output "/tmp/fitting_base.py"

          # Create the runnable script with proper imports
          cat > /tmp/fitting.py << 'PYEOF'
          import sys
          import os

          # Add the repository root to Python path so frontend can be imported
          sys.path.insert(0, '${{ env.WORKDIR }}')
          sys.path.insert(0, '${{ env.WORKDIR }}/frontend')

          # Set environment variables if needed
          os.environ['PYTHONPATH'] = '${{ env.WORKDIR }}:${{ env.WORKDIR }}/frontend:' + os.environ.get('PYTHONPATH', '')
          PYEOF

          # Append the converted notebook content
          cat "/tmp/fitting_base.py" >> /tmp/fitting.py

          # Run the example
          echo "fitting" > frontend/.CI
          python3 /tmp/fitting.py 2>&1 | tee /tmp/ci/fitting.log
          ENDSSH

      - name: Run friction
        run: |
          echo "Running friction..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} << 'ENDSSH'
          set -e
          cd ${{ env.WORKDIR }}

          # Activate Python environment
          source ~/.local/share/ppf-cts/venv/bin/activate

          # Convert notebook to Python script
          jupyter nbconvert --to python "examples/friction.ipynb" --output "/tmp/friction_base.py"

          # Create the runnable script with proper imports
          cat > /tmp/friction.py << 'PYEOF'
          import sys
          import os

          # Add the repository root to Python path so frontend can be imported
          sys.path.insert(0, '${{ env.WORKDIR }}')
          sys.path.insert(0, '${{ env.WORKDIR }}/frontend')

          # Set environment variables if needed
          os.environ['PYTHONPATH'] = '${{ env.WORKDIR }}:${{ env.WORKDIR }}/frontend:' + os.environ.get('PYTHONPATH', '')
          PYEOF

          # Append the converted notebook content
          cat "/tmp/friction_base.py" >> /tmp/friction.py

          # Run the example
          echo "friction" > frontend/.CI
          python3 /tmp/friction.py 2>&1 | tee /tmp/ci/friction.log
          ENDSSH



      - name: Collect results
        if: success() || failure()
        run: |
          echo "Collecting results from all runs..."
          mkdir -p ci
          rsync -avz --exclude='*.bin' --exclude='*.pickle' -e "ssh -p ${{ steps.ids.outputs.SSH_PORT }} -i ${{ steps.keypair.outputs.KEY_PATH }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null" \
            ${{ env.USER }}@${{ env.PUBLIC_IP }}:/tmp/ci/ ./ci/
          echo "## Collected Files:"
          ls -la ci/
          echo "## Run Summary:"
          for log in ci/*.log; do
            if [ -f "$log" ]; then
              echo "Found: $log"
            fi
          done

      - name: Upload artifact
        if: success() || failure()
        uses: actions/upload-artifact@v4
        with:
          name: ci-batch-2
          path: ci
          retention-days: 3

      - name: GPU information
        if: success() || failure()
        run: |
          echo "Getting GPU information..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} \
            "nvidia-smi" || echo "Failed to get GPU info"

      - name: Re-authenticate for cleanup
        if: always()
        continue-on-error: true
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Cleanup - Terminate Instance
        if: always()
        continue-on-error: true
        run: |
          if [ -n "${{ steps.instance.outputs.INSTANCE_ID }}" ]; then
            echo "Initiating instance termination: ${{ steps.instance.outputs.INSTANCE_ID }}"
            aws ec2 terminate-instances \
              --instance-ids "${{ steps.instance.outputs.INSTANCE_ID }}" \
              --region "$AWS_REGION" || true
            echo "Termination initiated. Instance will terminate in the background."
          else
            echo "No instance to terminate"
          fi

      - name: Cleanup - Remove Ingress Rules
        if: always()
        continue-on-error: true
        run: |
          if [ -n "${{ steps.security-group.outputs.SG_ID }}" ] && [ -n "${{ steps.security-group.outputs.RUNNER_IP_CIDR }}" ]; then
            echo "Removing ingress rules from security group ${{ steps.security-group.outputs.SG_ID }}"

            # Remove custom port rule
            if [ -n "${{ steps.security-group.outputs.SSH_PORT }}" ]; then
              echo "Removing port ${{ steps.security-group.outputs.SSH_PORT }} rule..."
              aws ec2 revoke-security-group-ingress \
                --group-id "${{ steps.security-group.outputs.SG_ID }}" \
                --ip-permissions \
                  "IpProtocol=tcp,FromPort=${{ steps.security-group.outputs.SSH_PORT }},ToPort=${{ steps.security-group.outputs.SSH_PORT }},IpRanges=[{CidrIp=${{ steps.security-group.outputs.RUNNER_IP_CIDR }}}]" \
                --region "$AWS_REGION" 2>&1 || echo "Note: Custom port rule may have already been removed"
            fi

            echo "Ingress rule removed successfully"
            echo "Security group ${{ steps.security-group.outputs.SG_ID }} remains for future use"
          else
            echo "No ingress rules to remove"
          fi

      - name: Cleanup - Remove Local SSH Key
        if: always()
        continue-on-error: true
        run: |
          if [ -n "${{ steps.keypair.outputs.KEY_PATH }}" ] && [ -f "${{ steps.keypair.outputs.KEY_PATH }}" ]; then
            rm -f "${{ steps.keypair.outputs.KEY_PATH }}"
            echo "Local SSH key file removed"
          fi

      - name: Summary
        if: always()
        run: |
          echo "## Workflow Summary - Batch 2"
          echo "- Region: $AWS_REGION"
          echo "- Instance Type: $INSTANCE_TYPE"
          echo "- Branch: $BRANCH"
          echo "- Examples: $EXAMPLES"
          echo "- Instance ID: ${{ steps.instance.outputs.INSTANCE_ID || 'Not launched' }}"
          echo "- Run Status: ${{ steps.run-examples.outcome || 'Not run' }}"

  run-batch-3:
    name: Run Batch 3
    runs-on: ubuntu-latest
    permissions:
      id-token: write
      contents: read

    env:
      AWS_REGION: ${{ github.event.inputs.region }}
      INSTANCE_TYPE: ${{ github.event.inputs.instance_type }}
      BRANCH: ${{ github.ref_name }}
      EXAMPLES: "hang needle noodle ribbon"
      WORKDIR: /home/ubuntu
      USER: ubuntu

    steps:
      - name: Show input parameters
        run: |
          echo "## Input Parameters - Batch 3"
          echo "Branch: ${{ github.ref_name }}"
          echo "Instance Type: ${{ github.event.inputs.instance_type }}"
          echo "Region: ${{ github.event.inputs.region }}"
          echo "Examples: hang needle noodle ribbon"

      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Verify AWS authentication
        run: |
          echo "Testing AWS authentication..."
          aws sts get-caller-identity
          echo "AWS Region: $AWS_REGION"
          echo "Instance Type: $INSTANCE_TYPE"
          echo "Branch: $BRANCH"
          echo "Examples: $EXAMPLES"

      - name: Get GitHub Actions runner public IP
        id: runner-ip
        run: |
          echo "Fetching GitHub Actions runner public IP..."
          RUNNER_IP=$(curl -s --max-time 10 https://checkip.amazonaws.com | tr -d '\n')
          if [ -z "$RUNNER_IP" ]; then
            echo "ERROR: Failed to get IP from checkip.amazonaws.com"
            exit 1
          fi
          echo "::add-mask::$RUNNER_IP"
          echo "RUNNER_IP=$RUNNER_IP" >> $GITHUB_OUTPUT
          echo "GitHub Actions Runner IP: $RUNNER_IP"

      - name: Find Deep Learning AMI
        id: ami
        run: |
          echo "Finding latest Deep Learning AMI with GPU support..."
          AMI_ID=$(aws ec2 describe-images \
            --owners amazon \
            --filters \
              "Name=name,Values=Deep Learning Base OSS Nvidia Driver GPU AMI (Ubuntu 24.04)*" \
              "Name=state,Values=available" \
              "Name=architecture,Values=x86_64" \
            --query 'sort_by(Images, &CreationDate)[-1].ImageId' \
            --region "$AWS_REGION" \
            --output text)

          if [ "$AMI_ID" = "None" ] || [ -z "$AMI_ID" ]; then
            echo "ERROR: Deep Learning AMI not found in region $AWS_REGION"
            echo "This workflow requires the Deep Learning AMI with pre-installed NVIDIA drivers"
            echo "Please check if the AMI is available in your selected region"
            exit 1
          fi

          echo "AMI_ID=$AMI_ID" >> $GITHUB_OUTPUT
          echo "Found AMI: $AMI_ID"

      - name: Get default VPC ID
        id: vpc
        run: |
          echo "Getting default VPC ID..."
          VPC_ID=$(aws ec2 describe-vpcs \
            --filters "Name=isDefault,Values=true" \
            --query 'Vpcs[0].VpcId' \
            --region "$AWS_REGION" \
            --output text)

          if [ "$VPC_ID" = "None" ] || [ -z "$VPC_ID" ]; then
            echo "ERROR: Default VPC not found in region $AWS_REGION"
            exit 1
          fi

          echo "VPC_ID=$VPC_ID" >> $GITHUB_OUTPUT
          echo "Default VPC: $VPC_ID"

      - name: Generate unique identifiers
        id: ids
        run: |
          TIMESTAMP=$(date +%Y%m%d%H%M%S)
          RANDOM_SUFFIX=$(head /dev/urandom | tr -dc a-z0-9 | head -c 6)
          TEMP_INSTANCE_ID="temp-${TIMESTAMP}-${RANDOM_SUFFIX}"

          # Generate random SSH port (10001-65535)
          SSH_PORT=$((10001 + RANDOM % 55535))
          echo "::add-mask::$SSH_PORT"
          echo "TIMESTAMP=$TIMESTAMP" >> $GITHUB_OUTPUT
          echo "TEMP_INSTANCE_ID=$TEMP_INSTANCE_ID" >> $GITHUB_OUTPUT
          echo "SSH_PORT=$SSH_PORT" >> $GITHUB_OUTPUT
          echo "Temporary Instance ID: $TEMP_INSTANCE_ID"
          echo "SSH Port: $SSH_PORT"

      - name: Setup persistent security group
        id: security-group
        run: |
          echo "Setting up persistent security group 'github-actions-persistent'..."

          SG_NAME="github-actions-persistent"
          SG_DESCRIPTION="Persistent security group for GitHub Actions with dynamic rules"

          # Check if security group already exists
          SG_ID=$(aws ec2 describe-security-groups \
            --filters "Name=group-name,Values=$SG_NAME" \
            --query 'SecurityGroups[0].GroupId' \
            --region "$AWS_REGION" \
            --output text || echo "")

          if [ "$SG_ID" = "None" ] || [ -z "$SG_ID" ]; then
            echo "Security group does not exist. Creating new one..."

            # Create security group
            SG_ID=$(aws ec2 create-security-group \
              --group-name "$SG_NAME" \
              --description "$SG_DESCRIPTION" \
              --vpc-id "${{ steps.vpc.outputs.VPC_ID }}" \
              --query 'GroupId' \
              --region "$AWS_REGION" \
              --output text)

            echo "Security Group created: $SG_ID"

            # Tag the security group
            aws ec2 create-tags \
              --resources "$SG_ID" \
              --tags \
                "Key=Name,Value=$SG_NAME" \
                "Key=ManagedBy,Value=GitHubActions" \
                "Key=Purpose,Value=PersistentDynamicRules" \
                "Key=CreatedAt,Value=$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
              --region "$AWS_REGION"

            echo "Security Group tagged successfully"
          else
            echo "Using existing security group: $SG_ID"
          fi

          echo "SG_ID=$SG_ID" >> $GITHUB_OUTPUT

          # Add only custom SSH port (no port 22)
          echo "Adding ingress rule for runner IP on port ${{ steps.ids.outputs.SSH_PORT }}"
          aws ec2 authorize-security-group-ingress \
            --group-id "$SG_ID" \
            --ip-permissions \
              "IpProtocol=tcp,FromPort=${{ steps.ids.outputs.SSH_PORT }},ToPort=${{ steps.ids.outputs.SSH_PORT }},IpRanges=[{CidrIp=${{ steps.runner-ip.outputs.RUNNER_IP }}/32,Description='GHA Run ${{ github.run_id }} Port ${{ steps.ids.outputs.SSH_PORT }}'}]" \
            --region "$AWS_REGION" 2>&1 || echo "Note: Rule may already exist"

          echo "RUNNER_IP_CIDR=${{ steps.runner-ip.outputs.RUNNER_IP }}/32" >> $GITHUB_OUTPUT
          echo "SSH_PORT=${{ steps.ids.outputs.SSH_PORT }}" >> $GITHUB_OUTPUT
          echo "SSH ingress rule added successfully (custom port only)"

          RULE_COUNT=$(aws ec2 describe-security-groups \
            --group-ids "$SG_ID" \
            --query 'length(SecurityGroups[0].IpPermissions)' \
            --region "$AWS_REGION" \
            --output text)
          echo "Security group has $RULE_COUNT active ingress rule(s)"

      - name: Retrieve SSH key from Parameter Store
        id: keypair
        run: |
          echo "Retrieving SSH private key from AWS Systems Manager..."
          KEY_NAME="${{ secrets.AWS_KEY_PAIR_NAME }}"

          # Retrieve the SSH private key from Parameter Store
          aws ssm get-parameter \
            --name "/github-actions/ec2/ssh-key" \
            --with-decryption \
            --query 'Parameter.Value' \
            --region "$AWS_REGION" \
            --output text > /tmp/github-actions-ec2.pem

          chmod 600 /tmp/github-actions-ec2.pem
          echo "SSH key retrieved successfully"
          echo "KEY_PATH=/tmp/github-actions-ec2.pem" >> $GITHUB_OUTPUT

      - name: Create user data script
        run: |
          echo '#!/bin/bash' > /tmp/user-data.sh
          echo 'set -x' >> /tmp/user-data.sh
          echo 'exec > >(tee /var/log/user-data.log) 2>&1' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo 'echo "=== User Data Script Started ==="' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo '# Wait for system to be ready' >> /tmp/user-data.sh
          echo 'sleep 5' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo '# Create SSH privilege separation directory' >> /tmp/user-data.sh
          echo 'echo "Creating /run/sshd directory"' >> /tmp/user-data.sh
          echo 'mkdir -p /run/sshd' >> /tmp/user-data.sh
          echo 'chmod 0755 /run/sshd' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo '# Configure custom SSH port' >> /tmp/user-data.sh
          echo 'echo "Configuring SSH port to '"${{ steps.ids.outputs.SSH_PORT }}"'"' >> /tmp/user-data.sh
          echo 'perl -pi -e "s/^#?Port 22$/Port '"${{ steps.ids.outputs.SSH_PORT }}"'/" /etc/ssh/sshd_config' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo '# Ensure Port directive exists' >> /tmp/user-data.sh
          echo 'if ! grep -q "^Port '"${{ steps.ids.outputs.SSH_PORT }}"'" /etc/ssh/sshd_config; then' >> /tmp/user-data.sh
          echo '  echo "Port '"${{ steps.ids.outputs.SSH_PORT }}"'" >> /etc/ssh/sshd_config' >> /tmp/user-data.sh
          echo 'fi' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo 'echo "SSH config after modification:"' >> /tmp/user-data.sh
          echo 'grep "^Port" /etc/ssh/sshd_config' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo '# Disable systemd socket activation' >> /tmp/user-data.sh
          echo 'echo "Disabling socket activation"' >> /tmp/user-data.sh
          echo 'systemctl stop ssh.socket' >> /tmp/user-data.sh
          echo 'systemctl disable ssh.socket' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo '# Test SSH configuration' >> /tmp/user-data.sh
          echo 'echo "Testing SSH configuration"' >> /tmp/user-data.sh
          echo 'sshd -t' >> /tmp/user-data.sh
          echo 'if [ $? -eq 0 ]; then' >> /tmp/user-data.sh
          echo '  echo "SSH config valid, restarting SSH service"' >> /tmp/user-data.sh
          echo '  systemctl restart ssh.service' >> /tmp/user-data.sh
          echo '  sleep 2' >> /tmp/user-data.sh
          echo '  systemctl status ssh.service' >> /tmp/user-data.sh
          echo '  echo "Checking listening ports:"' >> /tmp/user-data.sh
          echo '  ss -tlnp | grep sshd || netstat -tlnp | grep sshd' >> /tmp/user-data.sh
          echo '  echo "SSH reconfiguration successful"' >> /tmp/user-data.sh
          echo 'else' >> /tmp/user-data.sh
          echo '  echo "ERROR: SSH config invalid"' >> /tmp/user-data.sh
          echo '  exit 1' >> /tmp/user-data.sh
          echo 'fi' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo '# Install Rust (needed for cargo build)' >> /tmp/user-data.sh
          echo 'curl --proto '"'"'=https'"'"' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y' >> /tmp/user-data.sh
          echo 'source "$HOME/.cargo/env"' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo '# Verify nvidia-smi is available' >> /tmp/user-data.sh
          echo 'if command -v nvidia-smi &> /dev/null; then' >> /tmp/user-data.sh
          echo '    echo "NVIDIA drivers confirmed"' >> /tmp/user-data.sh
          echo '    nvidia-smi' >> /tmp/user-data.sh
          echo 'else' >> /tmp/user-data.sh
          echo '    echo "Warning: nvidia-smi not found"' >> /tmp/user-data.sh
          echo 'fi' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo '# Create workspace directory' >> /tmp/user-data.sh
          echo 'mkdir -p ${{ env.WORKDIR }}/workspace' >> /tmp/user-data.sh
          echo 'chown -R ${{ env.USER }}:${{ env.USER }} ${{ env.WORKDIR }}/workspace' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo 'nvidia-smi | tee /tmp/nvidia-smi-output.txt' >> /tmp/user-data.sh
          echo 'touch /tmp/setup-complete' >> /tmp/user-data.sh
          echo 'echo "=== User Data Script Complete ==="' >> /tmp/user-data.sh

      - name: Launch EC2 instance
        id: instance
        run: |
          echo "Launching EC2 instance with SSH configured on port ${{ steps.ids.outputs.SSH_PORT }}..."

          # Base64 encode for AWS
          USER_DATA=$(base64 -w 0 /tmp/user-data.sh)

          INSTANCE_ID=$(aws ec2 run-instances \
            --image-id "${{ steps.ami.outputs.AMI_ID }}" \
            --instance-type "$INSTANCE_TYPE" \
            --key-name "${{ secrets.AWS_KEY_PAIR_NAME }}" \
            --security-group-ids "${{ steps.security-group.outputs.SG_ID }}" \
            --user-data "$USER_DATA" \
            --block-device-mappings "DeviceName=/dev/sda1,Ebs={VolumeSize=256,VolumeType=gp3,DeleteOnTermination=true}" \
            --tag-specifications \
              "ResourceType=instance,Tags=[\
                {Key=Name,Value=gpu-runner-batch-3-${{ steps.ids.outputs.TIMESTAMP }}},\
                {Key=ManagedBy,Value=GitHubActions},\
                {Key=Purpose,Value=GPURunner},\
                {Key=Workflow,Value=${{ github.workflow }}},\
                {Key=RunId,Value=${{ github.run_id }}},\
                {Key=Branch,Value=${{ env.BRANCH }}},\
                {Key=Batch,Value=3}\
              ]" \
              "ResourceType=volume,Tags=[\
                {Key=Name,Value=gpu-runner-batch-3-${{ steps.ids.outputs.TIMESTAMP }}-volume},\
                {Key=ManagedBy,Value=GitHubActions},\
                {Key=Purpose,Value=GPURunner},\
                {Key=Workflow,Value=${{ github.workflow }}},\
                {Key=Batch,Value=3}\
              ]" \
            --instance-initiated-shutdown-behavior terminate \
            --query 'Instances[0].InstanceId' \
            --region "$AWS_REGION" \
            --output text)

          echo "INSTANCE_ID=$INSTANCE_ID" >> $GITHUB_OUTPUT
          echo "Instance launched: $INSTANCE_ID"

      - name: Wait for instance to be running
        run: |
          echo "Waiting for instance to be running..."
          aws ec2 wait instance-running \
            --instance-ids "${{ steps.instance.outputs.INSTANCE_ID }}" \
            --region "$AWS_REGION"

          PUBLIC_IP=$(aws ec2 describe-instances \
            --instance-ids "${{ steps.instance.outputs.INSTANCE_ID }}" \
            --query 'Reservations[0].Instances[0].PublicIpAddress' \
            --region "$AWS_REGION" \
            --output text)

          echo "::add-mask::$PUBLIC_IP"
          echo "PUBLIC_IP=$PUBLIC_IP" >> $GITHUB_ENV
          echo "Instance is running at: $PUBLIC_IP"

      - name: Wait for cloud-init and SSH on custom port
        run: |
          echo "Waiting for cloud-init to complete and SSH to be available on port ${{ steps.ids.outputs.SSH_PORT }}..."

          # Wait longer initially to allow cloud-init to run
          echo "Waiting 60 seconds for cloud-init to start..."
          sleep 60

          MAX_ATTEMPTS=40
          ATTEMPT=0

          while [ $ATTEMPT -lt $MAX_ATTEMPTS ]; do
            if ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o ConnectTimeout=5 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
              -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} "echo 'SSH ready on custom port'" 2>/dev/null; then
              echo "SSH connection established on port ${{ steps.ids.outputs.SSH_PORT }}"
              break
            else
              ATTEMPT=$((ATTEMPT + 1))
              if [ $ATTEMPT -eq $MAX_ATTEMPTS ]; then
                echo "Failed to establish SSH connection on port ${{ steps.ids.outputs.SSH_PORT }} after $MAX_ATTEMPTS attempts"
                echo "Attempting to fetch console output for debugging..."
                aws ec2 get-console-output \
                  --instance-id "${{ steps.instance.outputs.INSTANCE_ID }}" \
                  --region "$AWS_REGION" \
                  --output text || echo "Could not fetch console output"
                exit 1
              fi
              echo "Attempt $ATTEMPT/$MAX_ATTEMPTS failed, retrying in 10 seconds..."
              sleep 10
            fi
          done

      - name: Wait for instance setup
        run: |
          echo "Waiting for instance setup to complete..."
          MAX_WAIT=300
          ELAPSED=0

          while [ $ELAPSED -lt $MAX_WAIT ]; do
            if ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
              -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} \
              "test -f /tmp/setup-complete" 2>/dev/null; then
              echo "Instance setup completed"
              break
            else
              sleep 10
              ELAPSED=$((ELAPSED + 10))
              if [ $ELAPSED -ge $MAX_WAIT ]; then
                echo "Setup timeout, continuing anyway..."
                break
              fi
            fi
          done

      - name: Create archive of repository
        run: |
          echo "Creating repository archive..."
          git archive --format=tar.gz --output=/tmp/repo.tar.gz HEAD

      - name: Transfer repository to instance
        run: |
          echo "Transferring repository to instance..."
          scp -P ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" \
            /tmp/repo.tar.gz ${{ env.USER }}@${{ env.PUBLIC_IP }}:${{ env.WORKDIR }}/

          echo "Extracting repository on instance..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} \
            "cd ${{ env.WORKDIR }} && tar -xzf repo.tar.gz && rm repo.tar.gz"

      - name: Setup Python environment and run warmup
        run: |
          echo "Setting up Python environment and running warmup.py..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} << 'ENDSSH'
          set -e
          cd ${{ env.WORKDIR }}

          # Run warmup.py
          echo "Running warmup.py..."
          python3 warmup.py --skip-confirmation

          echo "Warmup completed"
          ENDSSH

      - name: Build Rust project
        run: |
          echo "Building Rust project with cargo..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} << 'ENDSSH'
          set -e
          cd ${{ env.WORKDIR }}

          # Setup Rust environment
          source "$HOME/.cargo/env"

          # Build the project
          echo "Running cargo build --release..."
          cargo build --release

          echo "Cargo build completed"
          ENDSSH

      - name: Setup CI directory
        run: |
          echo "Setting up CI directory..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} \
            "mkdir -p /tmp/ci"

      - name: Run hang
        run: |
          echo "Running hang..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} << 'ENDSSH'
          set -e
          cd ${{ env.WORKDIR }}

          # Activate Python environment
          source ~/.local/share/ppf-cts/venv/bin/activate

          # Convert notebook to Python script
          jupyter nbconvert --to python "examples/hang.ipynb" --output "/tmp/hang_base.py"

          # Create the runnable script with proper imports
          cat > /tmp/hang.py << 'PYEOF'
          import sys
          import os

          # Add the repository root to Python path so frontend can be imported
          sys.path.insert(0, '${{ env.WORKDIR }}')
          sys.path.insert(0, '${{ env.WORKDIR }}/frontend')

          # Set environment variables if needed
          os.environ['PYTHONPATH'] = '${{ env.WORKDIR }}:${{ env.WORKDIR }}/frontend:' + os.environ.get('PYTHONPATH', '')
          PYEOF

          # Append the converted notebook content
          cat "/tmp/hang_base.py" >> /tmp/hang.py

          # Run the example
          echo "hang" > frontend/.CI
          python3 /tmp/hang.py 2>&1 | tee /tmp/ci/hang.log
          ENDSSH

      - name: Run needle
        run: |
          echo "Running needle..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} << 'ENDSSH'
          set -e
          cd ${{ env.WORKDIR }}

          # Activate Python environment
          source ~/.local/share/ppf-cts/venv/bin/activate

          # Convert notebook to Python script
          jupyter nbconvert --to python "examples/needle.ipynb" --output "/tmp/needle_base.py"

          # Create the runnable script with proper imports
          cat > /tmp/needle.py << 'PYEOF'
          import sys
          import os

          # Add the repository root to Python path so frontend can be imported
          sys.path.insert(0, '${{ env.WORKDIR }}')
          sys.path.insert(0, '${{ env.WORKDIR }}/frontend')

          # Set environment variables if needed
          os.environ['PYTHONPATH'] = '${{ env.WORKDIR }}:${{ env.WORKDIR }}/frontend:' + os.environ.get('PYTHONPATH', '')
          PYEOF

          # Append the converted notebook content
          cat "/tmp/needle_base.py" >> /tmp/needle.py

          # Run the example
          echo "needle" > frontend/.CI
          python3 /tmp/needle.py 2>&1 | tee /tmp/ci/needle.log
          ENDSSH

      - name: Run noodle
        run: |
          echo "Running noodle..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} << 'ENDSSH'
          set -e
          cd ${{ env.WORKDIR }}

          # Activate Python environment
          source ~/.local/share/ppf-cts/venv/bin/activate

          # Convert notebook to Python script
          jupyter nbconvert --to python "examples/noodle.ipynb" --output "/tmp/noodle_base.py"

          # Create the runnable script with proper imports
          cat > /tmp/noodle.py << 'PYEOF'
          import sys
          import os

          # Add the repository root to Python path so frontend can be imported
          sys.path.insert(0, '${{ env.WORKDIR }}')
          sys.path.insert(0, '${{ env.WORKDIR }}/frontend')

          # Set environment variables if needed
          os.environ['PYTHONPATH'] = '${{ env.WORKDIR }}:${{ env.WORKDIR }}/frontend:' + os.environ.get('PYTHONPATH', '')
          PYEOF

          # Append the converted notebook content
          cat "/tmp/noodle_base.py" >> /tmp/noodle.py

          # Run the example
          echo "noodle" > frontend/.CI
          python3 /tmp/noodle.py 2>&1 | tee /tmp/ci/noodle.log
          ENDSSH

      - name: Run ribbon
        run: |
          echo "Running ribbon..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} << 'ENDSSH'
          set -e
          cd ${{ env.WORKDIR }}

          # Activate Python environment
          source ~/.local/share/ppf-cts/venv/bin/activate

          # Convert notebook to Python script
          jupyter nbconvert --to python "examples/ribbon.ipynb" --output "/tmp/ribbon_base.py"

          # Create the runnable script with proper imports
          cat > /tmp/ribbon.py << 'PYEOF'
          import sys
          import os

          # Add the repository root to Python path so frontend can be imported
          sys.path.insert(0, '${{ env.WORKDIR }}')
          sys.path.insert(0, '${{ env.WORKDIR }}/frontend')

          # Set environment variables if needed
          os.environ['PYTHONPATH'] = '${{ env.WORKDIR }}:${{ env.WORKDIR }}/frontend:' + os.environ.get('PYTHONPATH', '')
          PYEOF

          # Append the converted notebook content
          cat "/tmp/ribbon_base.py" >> /tmp/ribbon.py

          # Run the example
          echo "ribbon" > frontend/.CI
          python3 /tmp/ribbon.py 2>&1 | tee /tmp/ci/ribbon.log
          ENDSSH



      - name: Collect results
        if: success() || failure()
        run: |
          echo "Collecting results from all runs..."
          mkdir -p ci
          rsync -avz --exclude='*.bin' --exclude='*.pickle' -e "ssh -p ${{ steps.ids.outputs.SSH_PORT }} -i ${{ steps.keypair.outputs.KEY_PATH }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null" \
            ${{ env.USER }}@${{ env.PUBLIC_IP }}:/tmp/ci/ ./ci/
          echo "## Collected Files:"
          ls -la ci/
          echo "## Run Summary:"
          for log in ci/*.log; do
            if [ -f "$log" ]; then
              echo "Found: $log"
            fi
          done

      - name: Upload artifact
        if: success() || failure()
        uses: actions/upload-artifact@v4
        with:
          name: ci-batch-3
          path: ci
          retention-days: 3

      - name: GPU information
        if: success() || failure()
        run: |
          echo "Getting GPU information..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} \
            "nvidia-smi" || echo "Failed to get GPU info"

      - name: Re-authenticate for cleanup
        if: always()
        continue-on-error: true
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Cleanup - Terminate Instance
        if: always()
        continue-on-error: true
        run: |
          if [ -n "${{ steps.instance.outputs.INSTANCE_ID }}" ]; then
            echo "Initiating instance termination: ${{ steps.instance.outputs.INSTANCE_ID }}"
            aws ec2 terminate-instances \
              --instance-ids "${{ steps.instance.outputs.INSTANCE_ID }}" \
              --region "$AWS_REGION" || true
            echo "Termination initiated. Instance will terminate in the background."
          else
            echo "No instance to terminate"
          fi

      - name: Cleanup - Remove Ingress Rules
        if: always()
        continue-on-error: true
        run: |
          if [ -n "${{ steps.security-group.outputs.SG_ID }}" ] && [ -n "${{ steps.security-group.outputs.RUNNER_IP_CIDR }}" ]; then
            echo "Removing ingress rules from security group ${{ steps.security-group.outputs.SG_ID }}"

            # Remove custom port rule
            if [ -n "${{ steps.security-group.outputs.SSH_PORT }}" ]; then
              echo "Removing port ${{ steps.security-group.outputs.SSH_PORT }} rule..."
              aws ec2 revoke-security-group-ingress \
                --group-id "${{ steps.security-group.outputs.SG_ID }}" \
                --ip-permissions \
                  "IpProtocol=tcp,FromPort=${{ steps.security-group.outputs.SSH_PORT }},ToPort=${{ steps.security-group.outputs.SSH_PORT }},IpRanges=[{CidrIp=${{ steps.security-group.outputs.RUNNER_IP_CIDR }}}]" \
                --region "$AWS_REGION" 2>&1 || echo "Note: Custom port rule may have already been removed"
            fi

            echo "Ingress rule removed successfully"
            echo "Security group ${{ steps.security-group.outputs.SG_ID }} remains for future use"
          else
            echo "No ingress rules to remove"
          fi

      - name: Cleanup - Remove Local SSH Key
        if: always()
        continue-on-error: true
        run: |
          if [ -n "${{ steps.keypair.outputs.KEY_PATH }}" ] && [ -f "${{ steps.keypair.outputs.KEY_PATH }}" ]; then
            rm -f "${{ steps.keypair.outputs.KEY_PATH }}"
            echo "Local SSH key file removed"
          fi

      - name: Summary
        if: always()
        run: |
          echo "## Workflow Summary - Batch 3"
          echo "- Region: $AWS_REGION"
          echo "- Instance Type: $INSTANCE_TYPE"
          echo "- Branch: $BRANCH"
          echo "- Examples: $EXAMPLES"
          echo "- Instance ID: ${{ steps.instance.outputs.INSTANCE_ID || 'Not launched' }}"
          echo "- Run Status: ${{ steps.run-examples.outcome || 'Not run' }}"

  run-batch-4:
    name: Run Batch 4
    runs-on: ubuntu-latest
    permissions:
      id-token: write
      contents: read

    env:
      AWS_REGION: ${{ github.event.inputs.region }}
      INSTANCE_TYPE: ${{ github.event.inputs.instance_type }}
      BRANCH: ${{ github.ref_name }}
      EXAMPLES: "roller stack trampoline trapped"
      WORKDIR: /home/ubuntu
      USER: ubuntu

    steps:
      - name: Show input parameters
        run: |
          echo "## Input Parameters - Batch 4"
          echo "Branch: ${{ github.ref_name }}"
          echo "Instance Type: ${{ github.event.inputs.instance_type }}"
          echo "Region: ${{ github.event.inputs.region }}"
          echo "Examples: roller stack trampoline trapped"

      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Verify AWS authentication
        run: |
          echo "Testing AWS authentication..."
          aws sts get-caller-identity
          echo "AWS Region: $AWS_REGION"
          echo "Instance Type: $INSTANCE_TYPE"
          echo "Branch: $BRANCH"
          echo "Examples: $EXAMPLES"

      - name: Get GitHub Actions runner public IP
        id: runner-ip
        run: |
          echo "Fetching GitHub Actions runner public IP..."
          RUNNER_IP=$(curl -s --max-time 10 https://checkip.amazonaws.com | tr -d '\n')
          if [ -z "$RUNNER_IP" ]; then
            echo "ERROR: Failed to get IP from checkip.amazonaws.com"
            exit 1
          fi
          echo "::add-mask::$RUNNER_IP"
          echo "RUNNER_IP=$RUNNER_IP" >> $GITHUB_OUTPUT
          echo "GitHub Actions Runner IP: $RUNNER_IP"

      - name: Find Deep Learning AMI
        id: ami
        run: |
          echo "Finding latest Deep Learning AMI with GPU support..."
          AMI_ID=$(aws ec2 describe-images \
            --owners amazon \
            --filters \
              "Name=name,Values=Deep Learning Base OSS Nvidia Driver GPU AMI (Ubuntu 24.04)*" \
              "Name=state,Values=available" \
              "Name=architecture,Values=x86_64" \
            --query 'sort_by(Images, &CreationDate)[-1].ImageId' \
            --region "$AWS_REGION" \
            --output text)

          if [ "$AMI_ID" = "None" ] || [ -z "$AMI_ID" ]; then
            echo "ERROR: Deep Learning AMI not found in region $AWS_REGION"
            echo "This workflow requires the Deep Learning AMI with pre-installed NVIDIA drivers"
            echo "Please check if the AMI is available in your selected region"
            exit 1
          fi

          echo "AMI_ID=$AMI_ID" >> $GITHUB_OUTPUT
          echo "Found AMI: $AMI_ID"

      - name: Get default VPC ID
        id: vpc
        run: |
          echo "Getting default VPC ID..."
          VPC_ID=$(aws ec2 describe-vpcs \
            --filters "Name=isDefault,Values=true" \
            --query 'Vpcs[0].VpcId' \
            --region "$AWS_REGION" \
            --output text)

          if [ "$VPC_ID" = "None" ] || [ -z "$VPC_ID" ]; then
            echo "ERROR: Default VPC not found in region $AWS_REGION"
            exit 1
          fi

          echo "VPC_ID=$VPC_ID" >> $GITHUB_OUTPUT
          echo "Default VPC: $VPC_ID"

      - name: Generate unique identifiers
        id: ids
        run: |
          TIMESTAMP=$(date +%Y%m%d%H%M%S)
          RANDOM_SUFFIX=$(head /dev/urandom | tr -dc a-z0-9 | head -c 6)
          TEMP_INSTANCE_ID="temp-${TIMESTAMP}-${RANDOM_SUFFIX}"

          # Generate random SSH port (10001-65535)
          SSH_PORT=$((10001 + RANDOM % 55535))
          echo "::add-mask::$SSH_PORT"
          echo "TIMESTAMP=$TIMESTAMP" >> $GITHUB_OUTPUT
          echo "TEMP_INSTANCE_ID=$TEMP_INSTANCE_ID" >> $GITHUB_OUTPUT
          echo "SSH_PORT=$SSH_PORT" >> $GITHUB_OUTPUT
          echo "Temporary Instance ID: $TEMP_INSTANCE_ID"
          echo "SSH Port: $SSH_PORT"

      - name: Setup persistent security group
        id: security-group
        run: |
          echo "Setting up persistent security group 'github-actions-persistent'..."

          SG_NAME="github-actions-persistent"
          SG_DESCRIPTION="Persistent security group for GitHub Actions with dynamic rules"

          # Check if security group already exists
          SG_ID=$(aws ec2 describe-security-groups \
            --filters "Name=group-name,Values=$SG_NAME" \
            --query 'SecurityGroups[0].GroupId' \
            --region "$AWS_REGION" \
            --output text || echo "")

          if [ "$SG_ID" = "None" ] || [ -z "$SG_ID" ]; then
            echo "Security group does not exist. Creating new one..."

            # Create security group
            SG_ID=$(aws ec2 create-security-group \
              --group-name "$SG_NAME" \
              --description "$SG_DESCRIPTION" \
              --vpc-id "${{ steps.vpc.outputs.VPC_ID }}" \
              --query 'GroupId' \
              --region "$AWS_REGION" \
              --output text)

            echo "Security Group created: $SG_ID"

            # Tag the security group
            aws ec2 create-tags \
              --resources "$SG_ID" \
              --tags \
                "Key=Name,Value=$SG_NAME" \
                "Key=ManagedBy,Value=GitHubActions" \
                "Key=Purpose,Value=PersistentDynamicRules" \
                "Key=CreatedAt,Value=$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
              --region "$AWS_REGION"

            echo "Security Group tagged successfully"
          else
            echo "Using existing security group: $SG_ID"
          fi

          echo "SG_ID=$SG_ID" >> $GITHUB_OUTPUT

          # Add only custom SSH port (no port 22)
          echo "Adding ingress rule for runner IP on port ${{ steps.ids.outputs.SSH_PORT }}"
          aws ec2 authorize-security-group-ingress \
            --group-id "$SG_ID" \
            --ip-permissions \
              "IpProtocol=tcp,FromPort=${{ steps.ids.outputs.SSH_PORT }},ToPort=${{ steps.ids.outputs.SSH_PORT }},IpRanges=[{CidrIp=${{ steps.runner-ip.outputs.RUNNER_IP }}/32,Description='GHA Run ${{ github.run_id }} Port ${{ steps.ids.outputs.SSH_PORT }}'}]" \
            --region "$AWS_REGION" 2>&1 || echo "Note: Rule may already exist"

          echo "RUNNER_IP_CIDR=${{ steps.runner-ip.outputs.RUNNER_IP }}/32" >> $GITHUB_OUTPUT
          echo "SSH_PORT=${{ steps.ids.outputs.SSH_PORT }}" >> $GITHUB_OUTPUT
          echo "SSH ingress rule added successfully (custom port only)"

          RULE_COUNT=$(aws ec2 describe-security-groups \
            --group-ids "$SG_ID" \
            --query 'length(SecurityGroups[0].IpPermissions)' \
            --region "$AWS_REGION" \
            --output text)
          echo "Security group has $RULE_COUNT active ingress rule(s)"

      - name: Retrieve SSH key from Parameter Store
        id: keypair
        run: |
          echo "Retrieving SSH private key from AWS Systems Manager..."
          KEY_NAME="${{ secrets.AWS_KEY_PAIR_NAME }}"

          # Retrieve the SSH private key from Parameter Store
          aws ssm get-parameter \
            --name "/github-actions/ec2/ssh-key" \
            --with-decryption \
            --query 'Parameter.Value' \
            --region "$AWS_REGION" \
            --output text > /tmp/github-actions-ec2.pem

          chmod 600 /tmp/github-actions-ec2.pem
          echo "SSH key retrieved successfully"
          echo "KEY_PATH=/tmp/github-actions-ec2.pem" >> $GITHUB_OUTPUT

      - name: Create user data script
        run: |
          echo '#!/bin/bash' > /tmp/user-data.sh
          echo 'set -x' >> /tmp/user-data.sh
          echo 'exec > >(tee /var/log/user-data.log) 2>&1' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo 'echo "=== User Data Script Started ==="' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo '# Wait for system to be ready' >> /tmp/user-data.sh
          echo 'sleep 5' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo '# Create SSH privilege separation directory' >> /tmp/user-data.sh
          echo 'echo "Creating /run/sshd directory"' >> /tmp/user-data.sh
          echo 'mkdir -p /run/sshd' >> /tmp/user-data.sh
          echo 'chmod 0755 /run/sshd' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo '# Configure custom SSH port' >> /tmp/user-data.sh
          echo 'echo "Configuring SSH port to '"${{ steps.ids.outputs.SSH_PORT }}"'"' >> /tmp/user-data.sh
          echo 'perl -pi -e "s/^#?Port 22$/Port '"${{ steps.ids.outputs.SSH_PORT }}"'/" /etc/ssh/sshd_config' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo '# Ensure Port directive exists' >> /tmp/user-data.sh
          echo 'if ! grep -q "^Port '"${{ steps.ids.outputs.SSH_PORT }}"'" /etc/ssh/sshd_config; then' >> /tmp/user-data.sh
          echo '  echo "Port '"${{ steps.ids.outputs.SSH_PORT }}"'" >> /etc/ssh/sshd_config' >> /tmp/user-data.sh
          echo 'fi' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo 'echo "SSH config after modification:"' >> /tmp/user-data.sh
          echo 'grep "^Port" /etc/ssh/sshd_config' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo '# Disable systemd socket activation' >> /tmp/user-data.sh
          echo 'echo "Disabling socket activation"' >> /tmp/user-data.sh
          echo 'systemctl stop ssh.socket' >> /tmp/user-data.sh
          echo 'systemctl disable ssh.socket' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo '# Test SSH configuration' >> /tmp/user-data.sh
          echo 'echo "Testing SSH configuration"' >> /tmp/user-data.sh
          echo 'sshd -t' >> /tmp/user-data.sh
          echo 'if [ $? -eq 0 ]; then' >> /tmp/user-data.sh
          echo '  echo "SSH config valid, restarting SSH service"' >> /tmp/user-data.sh
          echo '  systemctl restart ssh.service' >> /tmp/user-data.sh
          echo '  sleep 2' >> /tmp/user-data.sh
          echo '  systemctl status ssh.service' >> /tmp/user-data.sh
          echo '  echo "Checking listening ports:"' >> /tmp/user-data.sh
          echo '  ss -tlnp | grep sshd || netstat -tlnp | grep sshd' >> /tmp/user-data.sh
          echo '  echo "SSH reconfiguration successful"' >> /tmp/user-data.sh
          echo 'else' >> /tmp/user-data.sh
          echo '  echo "ERROR: SSH config invalid"' >> /tmp/user-data.sh
          echo '  exit 1' >> /tmp/user-data.sh
          echo 'fi' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo '# Install Rust (needed for cargo build)' >> /tmp/user-data.sh
          echo 'curl --proto '"'"'=https'"'"' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y' >> /tmp/user-data.sh
          echo 'source "$HOME/.cargo/env"' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo '# Verify nvidia-smi is available' >> /tmp/user-data.sh
          echo 'if command -v nvidia-smi &> /dev/null; then' >> /tmp/user-data.sh
          echo '    echo "NVIDIA drivers confirmed"' >> /tmp/user-data.sh
          echo '    nvidia-smi' >> /tmp/user-data.sh
          echo 'else' >> /tmp/user-data.sh
          echo '    echo "Warning: nvidia-smi not found"' >> /tmp/user-data.sh
          echo 'fi' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo '# Create workspace directory' >> /tmp/user-data.sh
          echo 'mkdir -p ${{ env.WORKDIR }}/workspace' >> /tmp/user-data.sh
          echo 'chown -R ${{ env.USER }}:${{ env.USER }} ${{ env.WORKDIR }}/workspace' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo 'nvidia-smi | tee /tmp/nvidia-smi-output.txt' >> /tmp/user-data.sh
          echo 'touch /tmp/setup-complete' >> /tmp/user-data.sh
          echo 'echo "=== User Data Script Complete ==="' >> /tmp/user-data.sh

      - name: Launch EC2 instance
        id: instance
        run: |
          echo "Launching EC2 instance with SSH configured on port ${{ steps.ids.outputs.SSH_PORT }}..."

          # Base64 encode for AWS
          USER_DATA=$(base64 -w 0 /tmp/user-data.sh)

          INSTANCE_ID=$(aws ec2 run-instances \
            --image-id "${{ steps.ami.outputs.AMI_ID }}" \
            --instance-type "$INSTANCE_TYPE" \
            --key-name "${{ secrets.AWS_KEY_PAIR_NAME }}" \
            --security-group-ids "${{ steps.security-group.outputs.SG_ID }}" \
            --user-data "$USER_DATA" \
            --block-device-mappings "DeviceName=/dev/sda1,Ebs={VolumeSize=256,VolumeType=gp3,DeleteOnTermination=true}" \
            --tag-specifications \
              "ResourceType=instance,Tags=[\
                {Key=Name,Value=gpu-runner-batch-4-${{ steps.ids.outputs.TIMESTAMP }}},\
                {Key=ManagedBy,Value=GitHubActions},\
                {Key=Purpose,Value=GPURunner},\
                {Key=Workflow,Value=${{ github.workflow }}},\
                {Key=RunId,Value=${{ github.run_id }}},\
                {Key=Branch,Value=${{ env.BRANCH }}},\
                {Key=Batch,Value=4}\
              ]" \
              "ResourceType=volume,Tags=[\
                {Key=Name,Value=gpu-runner-batch-4-${{ steps.ids.outputs.TIMESTAMP }}-volume},\
                {Key=ManagedBy,Value=GitHubActions},\
                {Key=Purpose,Value=GPURunner},\
                {Key=Workflow,Value=${{ github.workflow }}},\
                {Key=Batch,Value=4}\
              ]" \
            --instance-initiated-shutdown-behavior terminate \
            --query 'Instances[0].InstanceId' \
            --region "$AWS_REGION" \
            --output text)

          echo "INSTANCE_ID=$INSTANCE_ID" >> $GITHUB_OUTPUT
          echo "Instance launched: $INSTANCE_ID"

      - name: Wait for instance to be running
        run: |
          echo "Waiting for instance to be running..."
          aws ec2 wait instance-running \
            --instance-ids "${{ steps.instance.outputs.INSTANCE_ID }}" \
            --region "$AWS_REGION"

          PUBLIC_IP=$(aws ec2 describe-instances \
            --instance-ids "${{ steps.instance.outputs.INSTANCE_ID }}" \
            --query 'Reservations[0].Instances[0].PublicIpAddress' \
            --region "$AWS_REGION" \
            --output text)

          echo "::add-mask::$PUBLIC_IP"
          echo "PUBLIC_IP=$PUBLIC_IP" >> $GITHUB_ENV
          echo "Instance is running at: $PUBLIC_IP"

      - name: Wait for cloud-init and SSH on custom port
        run: |
          echo "Waiting for cloud-init to complete and SSH to be available on port ${{ steps.ids.outputs.SSH_PORT }}..."

          # Wait longer initially to allow cloud-init to run
          echo "Waiting 60 seconds for cloud-init to start..."
          sleep 60

          MAX_ATTEMPTS=40
          ATTEMPT=0

          while [ $ATTEMPT -lt $MAX_ATTEMPTS ]; do
            if ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o ConnectTimeout=5 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
              -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} "echo 'SSH ready on custom port'" 2>/dev/null; then
              echo "SSH connection established on port ${{ steps.ids.outputs.SSH_PORT }}"
              break
            else
              ATTEMPT=$((ATTEMPT + 1))
              if [ $ATTEMPT -eq $MAX_ATTEMPTS ]; then
                echo "Failed to establish SSH connection on port ${{ steps.ids.outputs.SSH_PORT }} after $MAX_ATTEMPTS attempts"
                echo "Attempting to fetch console output for debugging..."
                aws ec2 get-console-output \
                  --instance-id "${{ steps.instance.outputs.INSTANCE_ID }}" \
                  --region "$AWS_REGION" \
                  --output text || echo "Could not fetch console output"
                exit 1
              fi
              echo "Attempt $ATTEMPT/$MAX_ATTEMPTS failed, retrying in 10 seconds..."
              sleep 10
            fi
          done

      - name: Wait for instance setup
        run: |
          echo "Waiting for instance setup to complete..."
          MAX_WAIT=300
          ELAPSED=0

          while [ $ELAPSED -lt $MAX_WAIT ]; do
            if ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
              -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} \
              "test -f /tmp/setup-complete" 2>/dev/null; then
              echo "Instance setup completed"
              break
            else
              sleep 10
              ELAPSED=$((ELAPSED + 10))
              if [ $ELAPSED -ge $MAX_WAIT ]; then
                echo "Setup timeout, continuing anyway..."
                break
              fi
            fi
          done

      - name: Create archive of repository
        run: |
          echo "Creating repository archive..."
          git archive --format=tar.gz --output=/tmp/repo.tar.gz HEAD

      - name: Transfer repository to instance
        run: |
          echo "Transferring repository to instance..."
          scp -P ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" \
            /tmp/repo.tar.gz ${{ env.USER }}@${{ env.PUBLIC_IP }}:${{ env.WORKDIR }}/

          echo "Extracting repository on instance..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} \
            "cd ${{ env.WORKDIR }} && tar -xzf repo.tar.gz && rm repo.tar.gz"

      - name: Setup Python environment and run warmup
        run: |
          echo "Setting up Python environment and running warmup.py..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} << 'ENDSSH'
          set -e
          cd ${{ env.WORKDIR }}

          # Run warmup.py
          echo "Running warmup.py..."
          python3 warmup.py --skip-confirmation

          echo "Warmup completed"
          ENDSSH

      - name: Build Rust project
        run: |
          echo "Building Rust project with cargo..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} << 'ENDSSH'
          set -e
          cd ${{ env.WORKDIR }}

          # Setup Rust environment
          source "$HOME/.cargo/env"

          # Build the project
          echo "Running cargo build --release..."
          cargo build --release

          echo "Cargo build completed"
          ENDSSH

      - name: Setup CI directory
        run: |
          echo "Setting up CI directory..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} \
            "mkdir -p /tmp/ci"

      - name: Run roller
        run: |
          echo "Running roller..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} << 'ENDSSH'
          set -e
          cd ${{ env.WORKDIR }}

          # Activate Python environment
          source ~/.local/share/ppf-cts/venv/bin/activate

          # Convert notebook to Python script
          jupyter nbconvert --to python "examples/roller.ipynb" --output "/tmp/roller_base.py"

          # Create the runnable script with proper imports
          cat > /tmp/roller.py << 'PYEOF'
          import sys
          import os

          # Add the repository root to Python path so frontend can be imported
          sys.path.insert(0, '${{ env.WORKDIR }}')
          sys.path.insert(0, '${{ env.WORKDIR }}/frontend')

          # Set environment variables if needed
          os.environ['PYTHONPATH'] = '${{ env.WORKDIR }}:${{ env.WORKDIR }}/frontend:' + os.environ.get('PYTHONPATH', '')
          PYEOF

          # Append the converted notebook content
          cat "/tmp/roller_base.py" >> /tmp/roller.py

          # Run the example
          echo "roller" > frontend/.CI
          python3 /tmp/roller.py 2>&1 | tee /tmp/ci/roller.log
          ENDSSH

      - name: Run stack
        run: |
          echo "Running stack..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} << 'ENDSSH'
          set -e
          cd ${{ env.WORKDIR }}

          # Activate Python environment
          source ~/.local/share/ppf-cts/venv/bin/activate

          # Convert notebook to Python script
          jupyter nbconvert --to python "examples/stack.ipynb" --output "/tmp/stack_base.py"

          # Create the runnable script with proper imports
          cat > /tmp/stack.py << 'PYEOF'
          import sys
          import os

          # Add the repository root to Python path so frontend can be imported
          sys.path.insert(0, '${{ env.WORKDIR }}')
          sys.path.insert(0, '${{ env.WORKDIR }}/frontend')

          # Set environment variables if needed
          os.environ['PYTHONPATH'] = '${{ env.WORKDIR }}:${{ env.WORKDIR }}/frontend:' + os.environ.get('PYTHONPATH', '')
          PYEOF

          # Append the converted notebook content
          cat "/tmp/stack_base.py" >> /tmp/stack.py

          # Run the example
          echo "stack" > frontend/.CI
          python3 /tmp/stack.py 2>&1 | tee /tmp/ci/stack.log
          ENDSSH

      - name: Run trampoline
        run: |
          echo "Running trampoline..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} << 'ENDSSH'
          set -e
          cd ${{ env.WORKDIR }}

          # Activate Python environment
          source ~/.local/share/ppf-cts/venv/bin/activate

          # Convert notebook to Python script
          jupyter nbconvert --to python "examples/trampoline.ipynb" --output "/tmp/trampoline_base.py"

          # Create the runnable script with proper imports
          cat > /tmp/trampoline.py << 'PYEOF'
          import sys
          import os

          # Add the repository root to Python path so frontend can be imported
          sys.path.insert(0, '${{ env.WORKDIR }}')
          sys.path.insert(0, '${{ env.WORKDIR }}/frontend')

          # Set environment variables if needed
          os.environ['PYTHONPATH'] = '${{ env.WORKDIR }}:${{ env.WORKDIR }}/frontend:' + os.environ.get('PYTHONPATH', '')
          PYEOF

          # Append the converted notebook content
          cat "/tmp/trampoline_base.py" >> /tmp/trampoline.py

          # Run the example
          echo "trampoline" > frontend/.CI
          python3 /tmp/trampoline.py 2>&1 | tee /tmp/ci/trampoline.log
          ENDSSH

      - name: Run trapped
        run: |
          echo "Running trapped..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} << 'ENDSSH'
          set -e
          cd ${{ env.WORKDIR }}

          # Activate Python environment
          source ~/.local/share/ppf-cts/venv/bin/activate

          # Convert notebook to Python script
          jupyter nbconvert --to python "examples/trapped.ipynb" --output "/tmp/trapped_base.py"

          # Create the runnable script with proper imports
          cat > /tmp/trapped.py << 'PYEOF'
          import sys
          import os

          # Add the repository root to Python path so frontend can be imported
          sys.path.insert(0, '${{ env.WORKDIR }}')
          sys.path.insert(0, '${{ env.WORKDIR }}/frontend')

          # Set environment variables if needed
          os.environ['PYTHONPATH'] = '${{ env.WORKDIR }}:${{ env.WORKDIR }}/frontend:' + os.environ.get('PYTHONPATH', '')
          PYEOF

          # Append the converted notebook content
          cat "/tmp/trapped_base.py" >> /tmp/trapped.py

          # Run the example
          echo "trapped" > frontend/.CI
          python3 /tmp/trapped.py 2>&1 | tee /tmp/ci/trapped.log
          ENDSSH



      - name: Collect results
        if: success() || failure()
        run: |
          echo "Collecting results from all runs..."
          mkdir -p ci
          rsync -avz --exclude='*.bin' --exclude='*.pickle' -e "ssh -p ${{ steps.ids.outputs.SSH_PORT }} -i ${{ steps.keypair.outputs.KEY_PATH }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null" \
            ${{ env.USER }}@${{ env.PUBLIC_IP }}:/tmp/ci/ ./ci/
          echo "## Collected Files:"
          ls -la ci/
          echo "## Run Summary:"
          for log in ci/*.log; do
            if [ -f "$log" ]; then
              echo "Found: $log"
            fi
          done

      - name: Upload artifact
        if: success() || failure()
        uses: actions/upload-artifact@v4
        with:
          name: ci-batch-4
          path: ci
          retention-days: 3

      - name: GPU information
        if: success() || failure()
        run: |
          echo "Getting GPU information..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} \
            "nvidia-smi" || echo "Failed to get GPU info"

      - name: Re-authenticate for cleanup
        if: always()
        continue-on-error: true
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Cleanup - Terminate Instance
        if: always()
        continue-on-error: true
        run: |
          if [ -n "${{ steps.instance.outputs.INSTANCE_ID }}" ]; then
            echo "Initiating instance termination: ${{ steps.instance.outputs.INSTANCE_ID }}"
            aws ec2 terminate-instances \
              --instance-ids "${{ steps.instance.outputs.INSTANCE_ID }}" \
              --region "$AWS_REGION" || true
            echo "Termination initiated. Instance will terminate in the background."
          else
            echo "No instance to terminate"
          fi

      - name: Cleanup - Remove Ingress Rules
        if: always()
        continue-on-error: true
        run: |
          if [ -n "${{ steps.security-group.outputs.SG_ID }}" ] && [ -n "${{ steps.security-group.outputs.RUNNER_IP_CIDR }}" ]; then
            echo "Removing ingress rules from security group ${{ steps.security-group.outputs.SG_ID }}"

            # Remove custom port rule
            if [ -n "${{ steps.security-group.outputs.SSH_PORT }}" ]; then
              echo "Removing port ${{ steps.security-group.outputs.SSH_PORT }} rule..."
              aws ec2 revoke-security-group-ingress \
                --group-id "${{ steps.security-group.outputs.SG_ID }}" \
                --ip-permissions \
                  "IpProtocol=tcp,FromPort=${{ steps.security-group.outputs.SSH_PORT }},ToPort=${{ steps.security-group.outputs.SSH_PORT }},IpRanges=[{CidrIp=${{ steps.security-group.outputs.RUNNER_IP_CIDR }}}]" \
                --region "$AWS_REGION" 2>&1 || echo "Note: Custom port rule may have already been removed"
            fi

            echo "Ingress rule removed successfully"
            echo "Security group ${{ steps.security-group.outputs.SG_ID }} remains for future use"
          else
            echo "No ingress rules to remove"
          fi

      - name: Cleanup - Remove Local SSH Key
        if: always()
        continue-on-error: true
        run: |
          if [ -n "${{ steps.keypair.outputs.KEY_PATH }}" ] && [ -f "${{ steps.keypair.outputs.KEY_PATH }}" ]; then
            rm -f "${{ steps.keypair.outputs.KEY_PATH }}"
            echo "Local SSH key file removed"
          fi

      - name: Summary
        if: always()
        run: |
          echo "## Workflow Summary - Batch 4"
          echo "- Region: $AWS_REGION"
          echo "- Instance Type: $INSTANCE_TYPE"
          echo "- Branch: $BRANCH"
          echo "- Examples: $EXAMPLES"
          echo "- Instance ID: ${{ steps.instance.outputs.INSTANCE_ID || 'Not launched' }}"
          echo "- Run Status: ${{ steps.run-examples.outcome || 'Not run' }}"

  run-batch-5:
    name: Run Batch 5
    runs-on: ubuntu-latest
    permissions:
      id-token: write
      contents: read

    env:
      AWS_REGION: ${{ github.event.inputs.region }}
      INSTANCE_TYPE: ${{ github.event.inputs.instance_type }}
      BRANCH: ${{ github.ref_name }}
      EXAMPLES: "twist five-twist woven yarn"
      WORKDIR: /home/ubuntu
      USER: ubuntu

    steps:
      - name: Show input parameters
        run: |
          echo "## Input Parameters - Batch 5"
          echo "Branch: ${{ github.ref_name }}"
          echo "Instance Type: ${{ github.event.inputs.instance_type }}"
          echo "Region: ${{ github.event.inputs.region }}"
          echo "Examples: twist five-twist woven yarn"

      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Verify AWS authentication
        run: |
          echo "Testing AWS authentication..."
          aws sts get-caller-identity
          echo "AWS Region: $AWS_REGION"
          echo "Instance Type: $INSTANCE_TYPE"
          echo "Branch: $BRANCH"
          echo "Examples: $EXAMPLES"

      - name: Get GitHub Actions runner public IP
        id: runner-ip
        run: |
          echo "Fetching GitHub Actions runner public IP..."
          RUNNER_IP=$(curl -s --max-time 10 https://checkip.amazonaws.com | tr -d '\n')
          if [ -z "$RUNNER_IP" ]; then
            echo "ERROR: Failed to get IP from checkip.amazonaws.com"
            exit 1
          fi
          echo "::add-mask::$RUNNER_IP"
          echo "RUNNER_IP=$RUNNER_IP" >> $GITHUB_OUTPUT
          echo "GitHub Actions Runner IP: $RUNNER_IP"

      - name: Find Deep Learning AMI
        id: ami
        run: |
          echo "Finding latest Deep Learning AMI with GPU support..."
          AMI_ID=$(aws ec2 describe-images \
            --owners amazon \
            --filters \
              "Name=name,Values=Deep Learning Base OSS Nvidia Driver GPU AMI (Ubuntu 24.04)*" \
              "Name=state,Values=available" \
              "Name=architecture,Values=x86_64" \
            --query 'sort_by(Images, &CreationDate)[-1].ImageId' \
            --region "$AWS_REGION" \
            --output text)

          if [ "$AMI_ID" = "None" ] || [ -z "$AMI_ID" ]; then
            echo "ERROR: Deep Learning AMI not found in region $AWS_REGION"
            echo "This workflow requires the Deep Learning AMI with pre-installed NVIDIA drivers"
            echo "Please check if the AMI is available in your selected region"
            exit 1
          fi

          echo "AMI_ID=$AMI_ID" >> $GITHUB_OUTPUT
          echo "Found AMI: $AMI_ID"

      - name: Get default VPC ID
        id: vpc
        run: |
          echo "Getting default VPC ID..."
          VPC_ID=$(aws ec2 describe-vpcs \
            --filters "Name=isDefault,Values=true" \
            --query 'Vpcs[0].VpcId' \
            --region "$AWS_REGION" \
            --output text)

          if [ "$VPC_ID" = "None" ] || [ -z "$VPC_ID" ]; then
            echo "ERROR: Default VPC not found in region $AWS_REGION"
            exit 1
          fi

          echo "VPC_ID=$VPC_ID" >> $GITHUB_OUTPUT
          echo "Default VPC: $VPC_ID"

      - name: Generate unique identifiers
        id: ids
        run: |
          TIMESTAMP=$(date +%Y%m%d%H%M%S)
          RANDOM_SUFFIX=$(head /dev/urandom | tr -dc a-z0-9 | head -c 6)
          TEMP_INSTANCE_ID="temp-${TIMESTAMP}-${RANDOM_SUFFIX}"

          # Generate random SSH port (10001-65535)
          SSH_PORT=$((10001 + RANDOM % 55535))
          echo "::add-mask::$SSH_PORT"
          echo "TIMESTAMP=$TIMESTAMP" >> $GITHUB_OUTPUT
          echo "TEMP_INSTANCE_ID=$TEMP_INSTANCE_ID" >> $GITHUB_OUTPUT
          echo "SSH_PORT=$SSH_PORT" >> $GITHUB_OUTPUT
          echo "Temporary Instance ID: $TEMP_INSTANCE_ID"
          echo "SSH Port: $SSH_PORT"

      - name: Setup persistent security group
        id: security-group
        run: |
          echo "Setting up persistent security group 'github-actions-persistent'..."

          SG_NAME="github-actions-persistent"
          SG_DESCRIPTION="Persistent security group for GitHub Actions with dynamic rules"

          # Check if security group already exists
          SG_ID=$(aws ec2 describe-security-groups \
            --filters "Name=group-name,Values=$SG_NAME" \
            --query 'SecurityGroups[0].GroupId' \
            --region "$AWS_REGION" \
            --output text || echo "")

          if [ "$SG_ID" = "None" ] || [ -z "$SG_ID" ]; then
            echo "Security group does not exist. Creating new one..."

            # Create security group
            SG_ID=$(aws ec2 create-security-group \
              --group-name "$SG_NAME" \
              --description "$SG_DESCRIPTION" \
              --vpc-id "${{ steps.vpc.outputs.VPC_ID }}" \
              --query 'GroupId' \
              --region "$AWS_REGION" \
              --output text)

            echo "Security Group created: $SG_ID"

            # Tag the security group
            aws ec2 create-tags \
              --resources "$SG_ID" \
              --tags \
                "Key=Name,Value=$SG_NAME" \
                "Key=ManagedBy,Value=GitHubActions" \
                "Key=Purpose,Value=PersistentDynamicRules" \
                "Key=CreatedAt,Value=$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
              --region "$AWS_REGION"

            echo "Security Group tagged successfully"
          else
            echo "Using existing security group: $SG_ID"
          fi

          echo "SG_ID=$SG_ID" >> $GITHUB_OUTPUT

          # Add only custom SSH port (no port 22)
          echo "Adding ingress rule for runner IP on port ${{ steps.ids.outputs.SSH_PORT }}"
          aws ec2 authorize-security-group-ingress \
            --group-id "$SG_ID" \
            --ip-permissions \
              "IpProtocol=tcp,FromPort=${{ steps.ids.outputs.SSH_PORT }},ToPort=${{ steps.ids.outputs.SSH_PORT }},IpRanges=[{CidrIp=${{ steps.runner-ip.outputs.RUNNER_IP }}/32,Description='GHA Run ${{ github.run_id }} Port ${{ steps.ids.outputs.SSH_PORT }}'}]" \
            --region "$AWS_REGION" 2>&1 || echo "Note: Rule may already exist"

          echo "RUNNER_IP_CIDR=${{ steps.runner-ip.outputs.RUNNER_IP }}/32" >> $GITHUB_OUTPUT
          echo "SSH_PORT=${{ steps.ids.outputs.SSH_PORT }}" >> $GITHUB_OUTPUT
          echo "SSH ingress rule added successfully (custom port only)"

          RULE_COUNT=$(aws ec2 describe-security-groups \
            --group-ids "$SG_ID" \
            --query 'length(SecurityGroups[0].IpPermissions)' \
            --region "$AWS_REGION" \
            --output text)
          echo "Security group has $RULE_COUNT active ingress rule(s)"

      - name: Retrieve SSH key from Parameter Store
        id: keypair
        run: |
          echo "Retrieving SSH private key from AWS Systems Manager..."
          KEY_NAME="${{ secrets.AWS_KEY_PAIR_NAME }}"

          # Retrieve the SSH private key from Parameter Store
          aws ssm get-parameter \
            --name "/github-actions/ec2/ssh-key" \
            --with-decryption \
            --query 'Parameter.Value' \
            --region "$AWS_REGION" \
            --output text > /tmp/github-actions-ec2.pem

          chmod 600 /tmp/github-actions-ec2.pem
          echo "SSH key retrieved successfully"
          echo "KEY_PATH=/tmp/github-actions-ec2.pem" >> $GITHUB_OUTPUT

      - name: Create user data script
        run: |
          echo '#!/bin/bash' > /tmp/user-data.sh
          echo 'set -x' >> /tmp/user-data.sh
          echo 'exec > >(tee /var/log/user-data.log) 2>&1' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo 'echo "=== User Data Script Started ==="' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo '# Wait for system to be ready' >> /tmp/user-data.sh
          echo 'sleep 5' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo '# Create SSH privilege separation directory' >> /tmp/user-data.sh
          echo 'echo "Creating /run/sshd directory"' >> /tmp/user-data.sh
          echo 'mkdir -p /run/sshd' >> /tmp/user-data.sh
          echo 'chmod 0755 /run/sshd' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo '# Configure custom SSH port' >> /tmp/user-data.sh
          echo 'echo "Configuring SSH port to '"${{ steps.ids.outputs.SSH_PORT }}"'"' >> /tmp/user-data.sh
          echo 'perl -pi -e "s/^#?Port 22$/Port '"${{ steps.ids.outputs.SSH_PORT }}"'/" /etc/ssh/sshd_config' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo '# Ensure Port directive exists' >> /tmp/user-data.sh
          echo 'if ! grep -q "^Port '"${{ steps.ids.outputs.SSH_PORT }}"'" /etc/ssh/sshd_config; then' >> /tmp/user-data.sh
          echo '  echo "Port '"${{ steps.ids.outputs.SSH_PORT }}"'" >> /etc/ssh/sshd_config' >> /tmp/user-data.sh
          echo 'fi' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo 'echo "SSH config after modification:"' >> /tmp/user-data.sh
          echo 'grep "^Port" /etc/ssh/sshd_config' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo '# Disable systemd socket activation' >> /tmp/user-data.sh
          echo 'echo "Disabling socket activation"' >> /tmp/user-data.sh
          echo 'systemctl stop ssh.socket' >> /tmp/user-data.sh
          echo 'systemctl disable ssh.socket' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo '# Test SSH configuration' >> /tmp/user-data.sh
          echo 'echo "Testing SSH configuration"' >> /tmp/user-data.sh
          echo 'sshd -t' >> /tmp/user-data.sh
          echo 'if [ $? -eq 0 ]; then' >> /tmp/user-data.sh
          echo '  echo "SSH config valid, restarting SSH service"' >> /tmp/user-data.sh
          echo '  systemctl restart ssh.service' >> /tmp/user-data.sh
          echo '  sleep 2' >> /tmp/user-data.sh
          echo '  systemctl status ssh.service' >> /tmp/user-data.sh
          echo '  echo "Checking listening ports:"' >> /tmp/user-data.sh
          echo '  ss -tlnp | grep sshd || netstat -tlnp | grep sshd' >> /tmp/user-data.sh
          echo '  echo "SSH reconfiguration successful"' >> /tmp/user-data.sh
          echo 'else' >> /tmp/user-data.sh
          echo '  echo "ERROR: SSH config invalid"' >> /tmp/user-data.sh
          echo '  exit 1' >> /tmp/user-data.sh
          echo 'fi' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo '# Install Rust (needed for cargo build)' >> /tmp/user-data.sh
          echo 'curl --proto '"'"'=https'"'"' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y' >> /tmp/user-data.sh
          echo 'source "$HOME/.cargo/env"' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo '# Verify nvidia-smi is available' >> /tmp/user-data.sh
          echo 'if command -v nvidia-smi &> /dev/null; then' >> /tmp/user-data.sh
          echo '    echo "NVIDIA drivers confirmed"' >> /tmp/user-data.sh
          echo '    nvidia-smi' >> /tmp/user-data.sh
          echo 'else' >> /tmp/user-data.sh
          echo '    echo "Warning: nvidia-smi not found"' >> /tmp/user-data.sh
          echo 'fi' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo '# Create workspace directory' >> /tmp/user-data.sh
          echo 'mkdir -p ${{ env.WORKDIR }}/workspace' >> /tmp/user-data.sh
          echo 'chown -R ${{ env.USER }}:${{ env.USER }} ${{ env.WORKDIR }}/workspace' >> /tmp/user-data.sh
          echo '' >> /tmp/user-data.sh
          echo 'nvidia-smi | tee /tmp/nvidia-smi-output.txt' >> /tmp/user-data.sh
          echo 'touch /tmp/setup-complete' >> /tmp/user-data.sh
          echo 'echo "=== User Data Script Complete ==="' >> /tmp/user-data.sh

      - name: Launch EC2 instance
        id: instance
        run: |
          echo "Launching EC2 instance with SSH configured on port ${{ steps.ids.outputs.SSH_PORT }}..."

          # Base64 encode for AWS
          USER_DATA=$(base64 -w 0 /tmp/user-data.sh)

          INSTANCE_ID=$(aws ec2 run-instances \
            --image-id "${{ steps.ami.outputs.AMI_ID }}" \
            --instance-type "$INSTANCE_TYPE" \
            --key-name "${{ secrets.AWS_KEY_PAIR_NAME }}" \
            --security-group-ids "${{ steps.security-group.outputs.SG_ID }}" \
            --user-data "$USER_DATA" \
            --block-device-mappings "DeviceName=/dev/sda1,Ebs={VolumeSize=256,VolumeType=gp3,DeleteOnTermination=true}" \
            --tag-specifications \
              "ResourceType=instance,Tags=[\
                {Key=Name,Value=gpu-runner-batch-5-${{ steps.ids.outputs.TIMESTAMP }}},\
                {Key=ManagedBy,Value=GitHubActions},\
                {Key=Purpose,Value=GPURunner},\
                {Key=Workflow,Value=${{ github.workflow }}},\
                {Key=RunId,Value=${{ github.run_id }}},\
                {Key=Branch,Value=${{ env.BRANCH }}},\
                {Key=Batch,Value=5}\
              ]" \
              "ResourceType=volume,Tags=[\
                {Key=Name,Value=gpu-runner-batch-5-${{ steps.ids.outputs.TIMESTAMP }}-volume},\
                {Key=ManagedBy,Value=GitHubActions},\
                {Key=Purpose,Value=GPURunner},\
                {Key=Workflow,Value=${{ github.workflow }}},\
                {Key=Batch,Value=5}\
              ]" \
            --instance-initiated-shutdown-behavior terminate \
            --query 'Instances[0].InstanceId' \
            --region "$AWS_REGION" \
            --output text)

          echo "INSTANCE_ID=$INSTANCE_ID" >> $GITHUB_OUTPUT
          echo "Instance launched: $INSTANCE_ID"

      - name: Wait for instance to be running
        run: |
          echo "Waiting for instance to be running..."
          aws ec2 wait instance-running \
            --instance-ids "${{ steps.instance.outputs.INSTANCE_ID }}" \
            --region "$AWS_REGION"

          PUBLIC_IP=$(aws ec2 describe-instances \
            --instance-ids "${{ steps.instance.outputs.INSTANCE_ID }}" \
            --query 'Reservations[0].Instances[0].PublicIpAddress' \
            --region "$AWS_REGION" \
            --output text)

          echo "::add-mask::$PUBLIC_IP"
          echo "PUBLIC_IP=$PUBLIC_IP" >> $GITHUB_ENV
          echo "Instance is running at: $PUBLIC_IP"

      - name: Wait for cloud-init and SSH on custom port
        run: |
          echo "Waiting for cloud-init to complete and SSH to be available on port ${{ steps.ids.outputs.SSH_PORT }}..."

          # Wait longer initially to allow cloud-init to run
          echo "Waiting 60 seconds for cloud-init to start..."
          sleep 60

          MAX_ATTEMPTS=40
          ATTEMPT=0

          while [ $ATTEMPT -lt $MAX_ATTEMPTS ]; do
            if ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o ConnectTimeout=5 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
              -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} "echo 'SSH ready on custom port'" 2>/dev/null; then
              echo "SSH connection established on port ${{ steps.ids.outputs.SSH_PORT }}"
              break
            else
              ATTEMPT=$((ATTEMPT + 1))
              if [ $ATTEMPT -eq $MAX_ATTEMPTS ]; then
                echo "Failed to establish SSH connection on port ${{ steps.ids.outputs.SSH_PORT }} after $MAX_ATTEMPTS attempts"
                echo "Attempting to fetch console output for debugging..."
                aws ec2 get-console-output \
                  --instance-id "${{ steps.instance.outputs.INSTANCE_ID }}" \
                  --region "$AWS_REGION" \
                  --output text || echo "Could not fetch console output"
                exit 1
              fi
              echo "Attempt $ATTEMPT/$MAX_ATTEMPTS failed, retrying in 10 seconds..."
              sleep 10
            fi
          done

      - name: Wait for instance setup
        run: |
          echo "Waiting for instance setup to complete..."
          MAX_WAIT=300
          ELAPSED=0

          while [ $ELAPSED -lt $MAX_WAIT ]; do
            if ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
              -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} \
              "test -f /tmp/setup-complete" 2>/dev/null; then
              echo "Instance setup completed"
              break
            else
              sleep 10
              ELAPSED=$((ELAPSED + 10))
              if [ $ELAPSED -ge $MAX_WAIT ]; then
                echo "Setup timeout, continuing anyway..."
                break
              fi
            fi
          done

      - name: Create archive of repository
        run: |
          echo "Creating repository archive..."
          git archive --format=tar.gz --output=/tmp/repo.tar.gz HEAD

      - name: Transfer repository to instance
        run: |
          echo "Transferring repository to instance..."
          scp -P ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" \
            /tmp/repo.tar.gz ${{ env.USER }}@${{ env.PUBLIC_IP }}:${{ env.WORKDIR }}/

          echo "Extracting repository on instance..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} \
            "cd ${{ env.WORKDIR }} && tar -xzf repo.tar.gz && rm repo.tar.gz"

      - name: Setup Python environment and run warmup
        run: |
          echo "Setting up Python environment and running warmup.py..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} << 'ENDSSH'
          set -e
          cd ${{ env.WORKDIR }}

          # Run warmup.py
          echo "Running warmup.py..."
          python3 warmup.py --skip-confirmation

          echo "Warmup completed"
          ENDSSH

      - name: Build Rust project
        run: |
          echo "Building Rust project with cargo..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} << 'ENDSSH'
          set -e
          cd ${{ env.WORKDIR }}

          # Setup Rust environment
          source "$HOME/.cargo/env"

          # Build the project
          echo "Running cargo build --release..."
          cargo build --release

          echo "Cargo build completed"
          ENDSSH

      - name: Setup CI directory
        run: |
          echo "Setting up CI directory..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} \
            "mkdir -p /tmp/ci"

      - name: Run twist
        run: |
          echo "Running twist..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} << 'ENDSSH'
          set -e
          cd ${{ env.WORKDIR }}

          # Activate Python environment
          source ~/.local/share/ppf-cts/venv/bin/activate

          # Convert notebook to Python script
          jupyter nbconvert --to python "examples/twist.ipynb" --output "/tmp/twist_base.py"

          # Create the runnable script with proper imports
          cat > /tmp/twist.py << 'PYEOF'
          import sys
          import os

          # Add the repository root to Python path so frontend can be imported
          sys.path.insert(0, '${{ env.WORKDIR }}')
          sys.path.insert(0, '${{ env.WORKDIR }}/frontend')

          # Set environment variables if needed
          os.environ['PYTHONPATH'] = '${{ env.WORKDIR }}:${{ env.WORKDIR }}/frontend:' + os.environ.get('PYTHONPATH', '')
          PYEOF

          # Append the converted notebook content
          cat "/tmp/twist_base.py" >> /tmp/twist.py

          # Run the example
          echo "twist" > frontend/.CI
          python3 /tmp/twist.py 2>&1 | tee /tmp/ci/twist.log
          ENDSSH

      - name: Run five-twist
        run: |
          echo "Running five-twist..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} << 'ENDSSH'
          set -e
          cd ${{ env.WORKDIR }}

          # Activate Python environment
          source ~/.local/share/ppf-cts/venv/bin/activate

          # Convert notebook to Python script
          jupyter nbconvert --to python "examples/five-twist.ipynb" --output "/tmp/five-twist_base.py"

          # Create the runnable script with proper imports
          cat > /tmp/five-twist.py << 'PYEOF'
          import sys
          import os

          # Add the repository root to Python path so frontend can be imported
          sys.path.insert(0, '${{ env.WORKDIR }}')
          sys.path.insert(0, '${{ env.WORKDIR }}/frontend')

          # Set environment variables if needed
          os.environ['PYTHONPATH'] = '${{ env.WORKDIR }}:${{ env.WORKDIR }}/frontend:' + os.environ.get('PYTHONPATH', '')
          PYEOF

          # Append the converted notebook content
          cat "/tmp/five-twist_base.py" >> /tmp/five-twist.py

          # Run the example
          echo "five-twist" > frontend/.CI
          python3 /tmp/five-twist.py 2>&1 | tee /tmp/ci/five-twist.log
          ENDSSH

      - name: Run woven
        run: |
          echo "Running woven..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} << 'ENDSSH'
          set -e
          cd ${{ env.WORKDIR }}

          # Activate Python environment
          source ~/.local/share/ppf-cts/venv/bin/activate

          # Convert notebook to Python script
          jupyter nbconvert --to python "examples/woven.ipynb" --output "/tmp/woven_base.py"

          # Create the runnable script with proper imports
          cat > /tmp/woven.py << 'PYEOF'
          import sys
          import os

          # Add the repository root to Python path so frontend can be imported
          sys.path.insert(0, '${{ env.WORKDIR }}')
          sys.path.insert(0, '${{ env.WORKDIR }}/frontend')

          # Set environment variables if needed
          os.environ['PYTHONPATH'] = '${{ env.WORKDIR }}:${{ env.WORKDIR }}/frontend:' + os.environ.get('PYTHONPATH', '')
          PYEOF

          # Append the converted notebook content
          cat "/tmp/woven_base.py" >> /tmp/woven.py

          # Run the example
          echo "woven" > frontend/.CI
          python3 /tmp/woven.py 2>&1 | tee /tmp/ci/woven.log
          ENDSSH

      - name: Run yarn
        run: |
          echo "Running yarn..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} << 'ENDSSH'
          set -e
          cd ${{ env.WORKDIR }}

          # Activate Python environment
          source ~/.local/share/ppf-cts/venv/bin/activate

          # Convert notebook to Python script
          jupyter nbconvert --to python "examples/yarn.ipynb" --output "/tmp/yarn_base.py"

          # Create the runnable script with proper imports
          cat > /tmp/yarn.py << 'PYEOF'
          import sys
          import os

          # Add the repository root to Python path so frontend can be imported
          sys.path.insert(0, '${{ env.WORKDIR }}')
          sys.path.insert(0, '${{ env.WORKDIR }}/frontend')

          # Set environment variables if needed
          os.environ['PYTHONPATH'] = '${{ env.WORKDIR }}:${{ env.WORKDIR }}/frontend:' + os.environ.get('PYTHONPATH', '')
          PYEOF

          # Append the converted notebook content
          cat "/tmp/yarn_base.py" >> /tmp/yarn.py

          # Run the example
          echo "yarn" > frontend/.CI
          python3 /tmp/yarn.py 2>&1 | tee /tmp/ci/yarn.log
          ENDSSH



      - name: Collect results
        if: success() || failure()
        run: |
          echo "Collecting results from all runs..."
          mkdir -p ci
          rsync -avz --exclude='*.bin' --exclude='*.pickle' -e "ssh -p ${{ steps.ids.outputs.SSH_PORT }} -i ${{ steps.keypair.outputs.KEY_PATH }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null" \
            ${{ env.USER }}@${{ env.PUBLIC_IP }}:/tmp/ci/ ./ci/
          echo "## Collected Files:"
          ls -la ci/
          echo "## Run Summary:"
          for log in ci/*.log; do
            if [ -f "$log" ]; then
              echo "Found: $log"
            fi
          done

      - name: Upload artifact
        if: success() || failure()
        uses: actions/upload-artifact@v4
        with:
          name: ci-batch-5
          path: ci
          retention-days: 3

      - name: GPU information
        if: success() || failure()
        run: |
          echo "Getting GPU information..."
          ssh -p ${{ steps.ids.outputs.SSH_PORT }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -i "${{ steps.keypair.outputs.KEY_PATH }}" ${{ env.USER }}@${{ env.PUBLIC_IP }} \
            "nvidia-smi" || echo "Failed to get GPU info"

      - name: Re-authenticate for cleanup
        if: always()
        continue-on-error: true
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Cleanup - Terminate Instance
        if: always()
        continue-on-error: true
        run: |
          if [ -n "${{ steps.instance.outputs.INSTANCE_ID }}" ]; then
            echo "Initiating instance termination: ${{ steps.instance.outputs.INSTANCE_ID }}"
            aws ec2 terminate-instances \
              --instance-ids "${{ steps.instance.outputs.INSTANCE_ID }}" \
              --region "$AWS_REGION" || true
            echo "Termination initiated. Instance will terminate in the background."
          else
            echo "No instance to terminate"
          fi

      - name: Cleanup - Remove Ingress Rules
        if: always()
        continue-on-error: true
        run: |
          if [ -n "${{ steps.security-group.outputs.SG_ID }}" ] && [ -n "${{ steps.security-group.outputs.RUNNER_IP_CIDR }}" ]; then
            echo "Removing ingress rules from security group ${{ steps.security-group.outputs.SG_ID }}"

            # Remove custom port rule
            if [ -n "${{ steps.security-group.outputs.SSH_PORT }}" ]; then
              echo "Removing port ${{ steps.security-group.outputs.SSH_PORT }} rule..."
              aws ec2 revoke-security-group-ingress \
                --group-id "${{ steps.security-group.outputs.SG_ID }}" \
                --ip-permissions \
                  "IpProtocol=tcp,FromPort=${{ steps.security-group.outputs.SSH_PORT }},ToPort=${{ steps.security-group.outputs.SSH_PORT }},IpRanges=[{CidrIp=${{ steps.security-group.outputs.RUNNER_IP_CIDR }}}]" \
                --region "$AWS_REGION" 2>&1 || echo "Note: Custom port rule may have already been removed"
            fi

            echo "Ingress rule removed successfully"
            echo "Security group ${{ steps.security-group.outputs.SG_ID }} remains for future use"
          else
            echo "No ingress rules to remove"
          fi

      - name: Cleanup - Remove Local SSH Key
        if: always()
        continue-on-error: true
        run: |
          if [ -n "${{ steps.keypair.outputs.KEY_PATH }}" ] && [ -f "${{ steps.keypair.outputs.KEY_PATH }}" ]; then
            rm -f "${{ steps.keypair.outputs.KEY_PATH }}"
            echo "Local SSH key file removed"
          fi

      - name: Summary
        if: always()
        run: |
          echo "## Workflow Summary - Batch 5"
          echo "- Region: $AWS_REGION"
          echo "- Instance Type: $INSTANCE_TYPE"
          echo "- Branch: $BRANCH"
          echo "- Examples: $EXAMPLES"
          echo "- Instance ID: ${{ steps.instance.outputs.INSTANCE_ID || 'Not launched' }}"
          echo "- Run Status: ${{ steps.run-examples.outcome || 'Not run' }}"

